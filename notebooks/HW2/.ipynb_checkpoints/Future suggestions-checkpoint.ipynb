{
 "metadata": {
  "name": "",
  "signature": "sha256:b7aaa1fec9e7db32fe24ddf4b5364f828b322e65ac8b813f67774485e6a91dc5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###The data-science style of coding###\n",
      "1. Your goal is to analyze the data, the code is just a means to an end.\n",
      "1. You are a Data Scientist, not just a programmer.\n",
      "1. Use short cycles: \n",
      "    1. Start a notebook, \n",
      "    1. write a bit of code, \n",
      "    1. run on data, \n",
      "    1. visualize results, \n",
      "    1. write conclusions. \n",
      "    1. Package code into functions/classes and write into .py files\n",
      "    1. Write numerical results into a PKL file with a dictionary that hold the variables of interest. \n",
      "        \n",
      "###Specific comments based on HW2 submissions###\n",
      "1. To avoid very skinny or very wide regions: first partition into fixed size squares(say 10 latitude and 10 longitude) and then partition each of the squares (if it has enough measurements) into smaller pieces.\n",
      "1. Be sure you are calculating percentage of variance explained correctly. For a single station the mean should already explain most of the variance. Look for the knee in the graph for each group. Separate out the stations for which a small number of eigen-vectors explains a lot. As a rule of thumb, the number of eigenvectors you use should be smaller than 10.\n",
      "1. When merging, it can be ok to give up some of the error.\n",
      "1. Plot and try to interpret the mean/eigenvectors.\n",
      "1. Cosider the coefficients of the eigenvectors as a function of time and location. Are the coefficients of neighboring stations correlated? Is there a long term trend?\n",
      "1. To evaluate the quality of your model you can split the data into train and test. Construct the model using the training data and the check how well it fits using the test data. You can partition years or (better) partition stations.\n",
      "\n",
      "### On debugging ###\n",
      "1. I will create for you, for both of the datasets and both on HDFS and on S3, a sequence of files with randomly permuted random subsets of each file with sizes $1/2,1/4,1/8,...,1/1024$ of the original file size.\n",
      "1. Do the merging on a single large EC2 instance, not using hadoop.\n",
      "1. Before using EMR perform local test with sets of increasing size (go up to 100MB input file)\n",
      "1. When you use hadoop and it crashes or takes a very long time (more than 30 minutes) do the following:\n",
      "    1. scale down to the largest size you tried on local.\n",
      "    1. Use ECatch (remember to disable after error found and running in production)\n",
      "    1. Add log prints `sys.stderr.write('status message')`\n",
      "    1. Study the generated stderr files."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Useful code snippets"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Memory Leaks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If a map or reduce job crashes on large input the reason might be a memory leak. In a memory leak objects are allocated but not released, causing the taks to consume more and more memory over time. To check whether this is happening, add a call to \"resources\":"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile testMemUsage.py\n",
      "import resource,sys\n",
      "from numpy import random\n",
      "logfile=sys.stderr\n",
      "\n",
      "for i in range(9):\n",
      "    v=random.rand(10**i)\n",
      "    logfile.write('size of vector=%10d, memory size: %s bytes\\n'\\\n",
      "          %(len(v),resource.getrusage(resource.RUSAGE_SELF).ru_maxrss))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting testMemUsage.py\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python testMemUsage.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "size of vector=         1, memory size: 14196736 bytes\r\n",
        "size of vector=        10, memory size: 14196736 bytes\r\n",
        "size of vector=       100, memory size: 14196736 bytes\r\n",
        "size of vector=      1000, memory size: 14196736 bytes\r\n",
        "size of vector=     10000, memory size: 14278656 bytes\r\n",
        "size of vector=    100000, memory size: 15081472 bytes\r\n",
        "size of vector=   1000000, memory size: 23085056 bytes\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "size of vector=  10000000, memory size: 103088128 bytes\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "size of vector= 100000000, memory size: 903090176 bytes\r\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can see that the stand-alon python process requires about 14 MB of memory. When the vector size gets to 100,000 it consumes about 1MB (8 bytes per entry) and therefor there is a measureable effect on the total memory usage."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Timing and the benefits of Numpy Arrays###\n",
      "To check how long a single run of the mapper take, use the \"time\" package.\n",
      "\n",
      "Whenever possible, use Numpy, rather than python lists. It will make your program >100 times faster!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "a=random.rand(1000000)\n",
      "b=random.rand(1000000)\n",
      "t1=time.clock()\n",
      "for method in ['numpy','comprehension','for loop']:\n",
      "    t1=time.clock()    \n",
      "    if method=='numpy':\n",
      "        print dot(a,b),\n",
      "    elif method=='comprehension':\n",
      "        print sum([a[i]*b[i] for i in range(len(a))]),\n",
      "    elif method=='for loop':\n",
      "        Sum=0\n",
      "        for i in range(len(a)):\n",
      "            Sum+=a[i]*b[i]\n",
      "        print Sum,\n",
      "    t2=time.clock()    \n",
      "    print method,t2-t1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "249647.577209 numpy 0.002152\n",
        "249647.577209"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " comprehension 0.867562\n",
        "249647.577209"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " for loop 0.998075\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}