{
 "metadata": {
  "name": "",
  "signature": "sha256:f7b59e182438c480959d12fefd73f12d3c27867347b126e58d0a178341d24c2f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a name='top'></a>\n",
      "## Overview of the Workflow ##\n",
      "1. **Count the Number of Measurements**     \n",
      "    Count the number of valid records of TMIN and TMAX measurement for each station using MapReduce.\n",
      "    - Input: \n",
      "    \n",
      "      s3://lge-bucket/weather-data/weather.csv            \n",
      "      \n",
      "    - Output: \n",
      "    \n",
      "      [station-measurement-counts.txt](#station-measurement-counts)\n",
      "          Format: \"station-id\" number_of_TMIN_or_TMAX_records                  \n",
      "    - Implementation:\n",
      "    \n",
      "      [weather_data_parser.py](#weather_data_parser)\n",
      "      \n",
      "      [mr_station_measurement_count.py](#mr_station_measurement_count)\n",
      "      \n",
      "      [run_mr_station_measurement_count.sh](#run_mr_station_measurement_count)\n",
      "      <a name='top-2'></a>\n",
      "  \n",
      "2. **Join Stations and Weights** \n",
      "\n",
      "    Join the station data table with the number of valid records as a new column `weight'.\n",
      "    - Input: \n",
      "      \n",
      "      s3://Weather.GHNC/stations.pkl.gz\n",
      "              \n",
      "      [station-measurement-counts.csv](#station-measurement-counts)\n",
      "    - Output:\n",
      "    \n",
      "      [station-lat-lon-weight.csv](#station-lat-lon-weight)\n",
      "          Format: station-id, latitude, longitude, weight\n",
      "    - Implementation:\n",
      "    \n",
      "      [join_station_with_weight.py](#join_station_with_weight)\n",
      "      <a name='top-3'></a>\n",
      "      \n",
      "3. **Spatial Partitioning** \n",
      "\n",
      "    Run k-d tree partitioning for the stations based on the weighted station data.\n",
      "    - Input: \n",
      "    \n",
      "      [stations-lat-lon-weight.csv](#station-lat-lon-weight)\n",
      "    - Output: \n",
      "    \n",
      "      [partition-tree.csv](#partition-tree)\n",
      "          Format: node-id, coordinate (either 'lat' or 'lon'), threshold (value of latitude or longitude)\n",
      "          \n",
      "      [station-to-node.csv](#station-to-node)\n",
      "          Format: station-id, node-id\n",
      "      \n",
      "    - Implementation: [geo_partition.py](#geo_partition)\n",
      "      <a name='top-4'></a>\n",
      "      \n",
      "4. **Concat TMIN and TMAX vector** \n",
      "\n",
      "    For the same station the same year, concatenate TMIN (1 x 365) and TMAX (1 x 365) vector to form a 1 x 730 vector. This is done using MapReduce.\n",
      "    - Input: \n",
      "      \n",
      "      s3://lge-bucket/weather-data/weather.csv\n",
      "    - Output: \n",
      "      s3://lge-bucket/weather-data/weather-tminmax/\n",
      "          Format: station-id:year, [data (1 x 730)]     \n",
      "    - Implementation:\n",
      "      \n",
      "      [mr_weather_concat_tminmax.py](#mr_weather_concat_tminmax)\n",
      "            \n",
      "      [run_mr_weather_concat_tminmax.sh](#run_mr_weather_concat_tminmax)\n",
      "      <a name='top-5'></a>\n",
      "\n",
      "5. **Compute Node Descriptors** \n",
      "\n",
      "    Compute the descriptor for each node at different level (0-9, 0 means the root level, 9 means leaf level) using MapReduce.\n",
      "    - Input: \n",
      "      \n",
      "      s3://lge-bucket/weather-data/weather-tminmax/\n",
      "            \n",
      "      [station-to-node.csv](#station-to-node)\n",
      "    - Output: \n",
      "            \n",
      "      s3://lge-bucket/weather-data/node-descriptor-k-n-dl/ (withot vector data)\n",
      "          Format: node-id, k, num_of_samples, descriptor_length\n",
      "                \n",
      "      s3://lge-bucket/weather-data/node-descriptor/ (full)\n",
      "          Format: node-id, k, num_of_samples, descriptor_length, [mu], [D_k], [U_k]\n",
      "              mu is the mean vector, 1 x 730.\n",
      "              D_k is the first k singular values, 1 x k.\n",
      "              U_k is the first k eigen vectors, k x 1 x 730.\n",
      "    - Implementation:\n",
      "            \n",
      "      [geo_partition.py](#geo_partition): provides StationToNode data structure.\n",
      "      \n",
      "      [mr_weather_pca.py](#mr_weather_pca): [mr_weather_pca_usage](#mr_weather_pca_usage)\n",
      "      \n",
      "      [run_mr_weather_pca.sh](#run_mr_weather_pca)\n",
      "      <a name='top-6'></a>\n",
      "                \n",
      "6. **Merge Nodes based on Minimum Descriptor Length (MDL) Principle** \n",
      "\n",
      "    For given level, check each pair of children under the same parent, if the sum of descriptor length from the children is larger than the descriptor length of the parent, then the children should merge.\n",
      "    - Input: \n",
      "      \n",
      "      s3://lge-bucket/weather-data/node-descriptor-k-n-dl/\n",
      "      \n",
      "      [partition-tree.csv](#partition-tree)\n",
      "    - Output:\n",
      "      \n",
      "      [partition-tree-nid-coord-thres-k-n-dl-m.csv](#partition-tree-nid-coord-thres-k-n-dl-m):\n",
      "          Format: node-id, coord, thres, k, num_of_samples, descriptor_length, should_merge_to_parent_or_not ('0' or '1')\n",
      "    - Implementation:\n",
      "    \n",
      "      [partition_node_merge.py](#partition_node_merge)\n",
      "      <a name='top-7'></a>\n",
      "            \n",
      "7. **Visualize Results** \n",
      "\n",
      "    Draw the stations as markers on google map. The station under the same node are drawn using the same color.\n",
      "    - Input: \n",
      "    \n",
      "      [station-lat-lon-weight.csv](#station-lat-lon-weight)\n",
      "      \n",
      "      [station-to-node.csv](#station-to-node)\n",
      "      \n",
      "      [partition-tree-nid-coord-thres-k-n-dl-m.csv](#partition-tree-nid-coord-thres-k-n-dl-m)\n",
      "            \n",
      "    - Output: [http://lge88.github.io/weather-map/](http://lge88.github.io/weather-map/)\n",
      "    - Implementation: [https://github.com/lge88/weather-map](https://github.com/lge88/weather-map)           "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Results & Analysis ##\n",
      "\n",
      "Results are visualized in google map: [http://lge88.github.io/weather-map/](http://lge88.github.io/weather-map/). With the GUI, user can:\n",
      "\n",
      "- Toggle stations visibility.\n",
      "- Toggle node merge.\n",
      "- Choose the level to inspect. Level is a integer from 0 to 9. 0 means no partition. 9 is the finest partition.\n",
      "- Choose sample ratio. The portion of the whole stations file to view. Showing all 85284 stations on google map makes it realy slow.\n",
      "- Shuffle colors. Shuffle the color of markers randomly.\n",
      "\n",
      "To visualize the data, user need to prepare three files:\n",
      "\n",
      "- station-lat-lon-weight.csv\n",
      "        Format: station-id, latitude, longitude, weight\n",
      "            - station-id: id of the station, AJ000037756, for example.\n",
      "            - latitude: a float in [-90.0, +90.0]. \n",
      "            - longitude: a float in [-180.0, +180.0].\n",
      "            - weight: the number of valid records for certain measurement of the station.\n",
      "- station-to-node.csv\n",
      "        Format: node-id, station-id\n",
      "            - node-id: id of the partition node, consist of only '0' and '1'. Empty string means root.\n",
      "            - station-id: id of the station, AJ000037756, for example.\n",
      "- partition-tree-nid-coord-thres-k-n-dl-m.csv\n",
      "        Format: node-id, coord, thres, k, n, dl, m\n",
      "            - node-id: id of the partition node.\n",
      "            - coord: 'lat' or 'lon'.\n",
      "            - thres: value of latitude or longitude.\n",
      "            - k: number of eigen vectors.\n",
      "            - n: number of samples.\n",
      "            - dl: node descriptor length calculated by dl = n * k + (k + 1) * 730.\n",
      "            - m: a flag indicates whether the node should merged. '0' means no, '1' means yes.\n",
      "            \n",
      "## Difficultes in the Implementation ##\n",
      "\n",
      "The code works on both local and emr when processing the sampled datasets. But when it applies to the full size data, the tasks failed. Therefore the visualization of the results is based on the result of sampled dataset (1-of-100). \n",
      "\n",
      "By inspecting the error logs, there are two major reason causes the job to fail. \n",
      "\n",
      "The first one was the memory error. It occurs in the reducer of the first step. The cause was try to load all vector data into the memory when compute mean vector and then recenter each vector according to its mean vector.\n",
      "    # Pseudocode\n",
      "    vecs = list(vecs)\n",
      "    # vecs was a generator, but now it is a list fully loaded into the memory\n",
      "    # this might huge if the number of vecs are large\n",
      "    mu = compute_vec_mean(vecs)\n",
      "    for vec in vecs:\n",
      "        vec -= mu\n",
      "        yield node, vec\n",
      "       \n",
      "The issue was fixed by writing the vectors line by line into a temp file. After the mean vector is computed, read the vectors from file and process line by line.\n",
      "    # Pseudocode\n",
      "    mu = zero_vector()\n",
      "    n = 0\n",
      "    for vec in vecs:\n",
      "        write_vector_to(tmp_file, encode_vec(vec))\n",
      "        n += 1\n",
      "        mu += vec\n",
      "    mu /= n\n",
      "    \n",
      "    for line in read_vector_from(tmp_file):\n",
      "        vec = decode(line)\n",
      "        vec -= mu\n",
      "        yield node, vec\n",
      "    \n",
      "\n",
      "The second the issue might be the disk space limitation. This cause is not yet full identified. By the task error log, the following exceptions are found:\n",
      "\n",
      "    java.lang.RuntimeException: java.io.IOException: Spill failed\n",
      "\n",
      "By searching the web, [stack overflow](http://stackoverflow.com/questions/11602074/hadoop-job-runs-okay-on-smaller-set-of-data-but-fails-with-large-dataset) indicates the disk space limitation might be the cause. If that is the case, two ways might be tried in the future:\n",
      "\n",
      "- Compress before yielding the values\n",
      "- Choose machines with larger disk space"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Intermediate Results ###"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### station-measurement-counts.txt\n",
      "[back to top](#top) <a name='station-measurement-counts'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -10 data/station-measurement-counts.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\"AJ000037756\"\t4\r\n",
        "\"AJ000037907\"\t42\r\n",
        "\"AQC00914873\"\t18\r\n",
        "\"AR000087270\"\t95\r\n",
        "\"AR000087344\"\t103\r\n",
        "\"AR000087418\"\t100\r\n",
        "\"AR000087715\"\t106\r\n",
        "\"AR000087803\"\t97\r\n",
        "\"AR000870470\"\t94\r\n",
        "\"ASN00001019\"\t28\r\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### station-lat-lon-weight.csv\n",
      "[back to top](#top-2) <a name='station-lat-lon-weight'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -10 data/station-lat-lon-weight.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ACW00011604,17.1167,-61.7833,2\r\n",
        "ACW00011647,17.1333,-61.7833,0\r\n",
        "AE000041196,25.333,55.517,83\r\n",
        "AF000040930,35.317,69.017,3\r\n",
        "AG000060390,36.7167,3.25,146\r\n",
        "AG000060590,30.5667,2.8667,146\r\n",
        "AG000060611,28.05,9.6331,108\r\n",
        "AG000060680,22.8,5.4331,146\r\n",
        "AGE00135039,35.7297,0.65,126\r\n",
        "AJ000037575,41.55,46.667,71\r\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### partition-tree.csv\n",
      "[back to top](#top-3) <a name='partition-tree'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -10 data/partition-tree.csv\n",
      "!echo ...\n",
      "!tail -10 data/partition-tree.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ",lon,-90.7964\r\n",
        "0,lat,41.4917\r\n",
        "00,lon,-104.6822\r\n",
        "000,lat,37.0856\r\n",
        "0000,lon,-112.8003\r\n",
        "00000,lat,34.0697\r\n",
        "000000,lon,-117.9789\r\n",
        "0000000,lat,21.1539\r\n",
        "00000000,lon,-155.9117\r\n",
        "000000000,lat,20.7583\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "11111101,lon,66.05\r\n",
        "111111010,lat,65.967\r\n",
        "111111011,lat,67.433\r\n",
        "1111111,lat,62.833\r\n",
        "11111110,lon,129.717\r\n",
        "111111100,lat,59.283\r\n",
        "111111101,lat,59.917\r\n",
        "11111111,lon,136.217\r\n",
        "111111110,lat,67.55\r\n",
        "111111111,lat,65.733\r\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### station-to-node.csv\n",
      "[back to top](#top-3) <a name='station-to-node'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -10 data/station-to-node.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "USC00515000,000000000\r\n",
        "KRC00914101,000000000\r\n",
        "USC00514725,000000000\r\n",
        "AQC00914822,000000000\r\n",
        "USC00511008,000000000\r\n",
        "USC00517000,000000000\r\n",
        "JQW00021602,000000000\r\n",
        "USC00515006,000000000\r\n",
        "USC00512558,000000000\r\n",
        "AQC00914873,000000000\r\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### partition-tree-nid-coord-thres-k-n-dl-m.csv\n",
      "[back to top](#top-6) <a name='partition-tree-nid-coord-thres-k-n-dl-m'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -10 data/partition-tree-nid-coord-thres-k-n-dl-m-1-of-100.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ",lon,-90.7964,532,39230,21259450,0\r\n",
        "10110001,lon,-77.8167,103,124,88692,1\r\n",
        "110000,lon,-84.7667,367,668,513796,1\r\n",
        "110001,lon,-84.6333,371,644,510484,1\r\n",
        "10110000,lon,-77.9667,112,138,97946,1\r\n",
        "1101001,lat,43.98,187,244,182868,0\r\n",
        "010,lat,48.0667,530,4518,2782170,1\r\n",
        "01000000,lon,-123.3572,110,153,97860,0\r\n",
        "101011,lon,149.5242,247,420,284780,1\r\n",
        "110110100,lat,43.9522,72,85,59410,0\r\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Implementation ##"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### weather_data_parser.py\n",
      "[back to top](#top) <a name='weather_data_parser'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load weather_data_parser.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "class WeatherDataParser:\n",
      "  DAYS = 365\n",
      "\n",
      "  def __init__(self,\n",
      "               default_int_val = None,\n",
      "               good_data_ratio_threshold = 0.5,\n",
      "               enable_post_processing = True,\n",
      "               selected_days = None):\n",
      "    self.default_int_val = default_int_val\n",
      "    self.good_data_ratio_threshold = good_data_ratio_threshold\n",
      "    self.enable_post_processing = enable_post_processing\n",
      "\n",
      "    if selected_days is None:\n",
      "      self.selected_days = range(WeatherDataParser.DAYS)\n",
      "    else:\n",
      "      self.selected_days = set([day for day in selected_days if day >= 0 and day < WeatherDataParser.DAYS])\n",
      "\n",
      "  def parse_int(self, x):\n",
      "    try:\n",
      "      return int(x)\n",
      "    except ValueError:\n",
      "      return self.default_int_val\n",
      "\n",
      "  def post_process_data(self, data):\n",
      "    def find_nearest_neighbor(data, indx):\n",
      "      offset, days = 1, WeatherDataParser.DAYS\n",
      "      half_days = days / 2\n",
      "      while offset <= half_days:\n",
      "        r_indx = (indx + offset) % days\n",
      "        val = data[r_indx]\n",
      "        if val is not None: return val\n",
      "\n",
      "        l_indx = (indx - offset) % days\n",
      "        val = data[l_indx]\n",
      "        if val is not None: return val\n",
      "\n",
      "        offset += 1\n",
      "      return self.default_int_val\n",
      "      # raise Exception('Should never reach hear with proper good data ratio threshold!')\n",
      "\n",
      "    for i, x in enumerate(data):\n",
      "      if x is None:\n",
      "        data[i] = find_nearest_neighbor(data, i)\n",
      "\n",
      "  def parse_line(self, line):\n",
      "    vec = line.strip().split(',')\n",
      "    station = vec[0]\n",
      "    measurement = vec[1]\n",
      "    year = self.parse_int(vec[2])\n",
      "\n",
      "    data, num_of_records = [], 0\n",
      "\n",
      "    for day in self.selected_days:\n",
      "      item = vec[3 + day]\n",
      "      x = self.parse_int(item)\n",
      "      if x is not None: num_of_records += 1\n",
      "      data.append(x)\n",
      "\n",
      "    num_of_days = len(self.selected_days)\n",
      "\n",
      "    if num_of_days == 0 or float(num_of_records) / num_of_days < self.good_data_ratio_threshold:\n",
      "      return None\n",
      "    else:\n",
      "      if self.enable_post_processing: self.post_process_data(data)\n",
      "      return (station, year, measurement, data, num_of_records)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "  wdp = WeatherDataParser(selected_days=range(20))\n",
      "\n",
      "  with open('./weather-923-of-9358395-shuffled.csv') as f:\n",
      "    print 1, wdp.parse_line(f.readline())\n",
      "    print 2, wdp.parse_line(f.readline())\n",
      "    print 3, wdp.parse_line(f.readline())\n",
      "    print 4, wdp.parse_line(f.readline())\n",
      "    print 5, wdp.parse_line(f.readline())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### mr_station_measurement_count.py\n",
      "[back to top](#top) <a name='mr_station_measurement_count'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load mr_station_measurement_count.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "from mrjob.job import MRJob\n",
      "from weather_data_parser import WeatherDataParser\n",
      "\n",
      "class MRStationMeasurementCount(MRJob):\n",
      "  def mapper_init(self):\n",
      "    self.parser = WeatherDataParser()\n",
      "\n",
      "  def mapper(self, _, line):\n",
      "    res = self.parser.parse_line(line)\n",
      "    if res is not None:\n",
      "      station, year, measurement, data, num_of_records = res\n",
      "      if measurement == 'TMIN' or measurement == 'TMAX':\n",
      "        yield station, 1\n",
      "\n",
      "  def combiner(self, key, vals):\n",
      "    yield key, sum(vals)\n",
      "\n",
      "  def reducer(self, key, vals):\n",
      "    yield key, sum(vals)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "  MRStationMeasurementCount.run()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### run_mr_station_measurement_count.sh\n",
      "[back to top](#top) <a name='run_mr_station_measurement_count'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load run_mr_station_measurement_count.sh"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#! /bin/sh\n",
      "JOB_FLOW_ID='j-XF54NKBXHHP0'\n",
      "\n",
      "MODE=$1\n",
      "if [[ $MODE == '' ]]; then\n",
      "  MODE=local\n",
      "fi\n",
      "\n",
      "if [[ $MODE == 'emr' ]]; then\n",
      "  python mr_station_measurement_count.py -r emr \\\n",
      "    --emr-job-flow-id ${JOB_FLOW_ID} \\\n",
      "    -c ~/.mrjob.conf \\\n",
      "    s3://lge-bucket/weather-data/weather.csv > data/station-measurement-counts.txt\n",
      "else\n",
      "  python mr_station_measurement_count.py \\\n",
      "    data/weather-923-of-9358395-shuffled.csv\n",
      "fi"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### join_station_with_weight.py\n",
      "[back to top](#top-2) <a name='join_station_with_weight'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load join_station_with_weight.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import pickle, re\n",
      "\n",
      "def join_stations_measurement_counts(filename, stations):\n",
      "  LINE_RE = '^\"(.*)\"\\t(.*)$'\n",
      "  stations['weight'] = 0.0\n",
      "  with open(filename, 'r') as f:\n",
      "    for line in f.readlines():\n",
      "      m = re.match(LINE_RE, line.strip())\n",
      "      if m is not None:\n",
      "        station, counts = m.group(1), int(m.group(2))\n",
      "        if station in stations.index:\n",
      "          stations.loc[station, 'weight'] = counts\n",
      "  return df_to_list(stations[['latitude', 'longitude', 'weight']])\n",
      "\n",
      "def df_to_dict(df):\n",
      "  out = {}\n",
      "  for indx in df.index:\n",
      "    item = df.ix[indx]\n",
      "    out[indx] = (item.latitude, item.longitude, item.weight)\n",
      "  return out\n",
      "\n",
      "def df_to_list(df):\n",
      "  out = []\n",
      "  for indx in df.index:\n",
      "    item = df.ix[indx]\n",
      "    out.append((str(indx), float(item.latitude), float(item.longitude), int(item.weight)))\n",
      "  return out\n",
      "\n",
      "def export_pickle_data(obj, filename):\n",
      "  with open(filename, 'wb') as f:\n",
      "    pickle.dump(obj, f)\n",
      "\n",
      "def export_text_data(lst, filename):\n",
      "  with open(filename, 'wb') as f:\n",
      "    for item in lst:\n",
      "      f.write(','.join(map(str, item)));\n",
      "      f.write('\\n');\n",
      "\n",
      "def load_pickle_data(filename):\n",
      "  with open(filename, 'rb') as f:\n",
      "    return pickle.load(f)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "  s1 = load_pickle_data('stations.pkl')\n",
      "  s2 = join_stations_measurement_counts('data/station-measurement-counts.txt', s1)\n",
      "  export_text_data(s2, 'data/station-lat-lon-weight.csv')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### geo_partition.py\n",
      "[back to top](#top-3) <a name='geo_partition'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load geo_partition.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle, re, random\n",
      "\n",
      "class Station:\n",
      "  def __init__(self, name, lat, lon, weight):\n",
      "    self.name = str(name)\n",
      "    self.lat = float(lat)\n",
      "    self.lon = float(lon)\n",
      "    self.weight = int(weight)\n",
      "\n",
      "  def __repr__(self):\n",
      "    return '\\n'.join([\n",
      "      'name: ' + str(self.name),\n",
      "      'lat: ' + str(self.lat),\n",
      "      'lon: ' + str(self.lon),\n",
      "      'weight: ' + str(self.weight),\n",
      "    ])\n",
      "\n",
      "class StationToNodeTable:\n",
      "  def __init__(self):\n",
      "    self._table = {}\n",
      "\n",
      "  def read_from_csv_file(self, fname):\n",
      "    with open(fname, 'r') as f:\n",
      "      for line in f.readlines():\n",
      "        items = line.strip().split(',')\n",
      "        if len(items) == 2:\n",
      "          station, node = items\n",
      "          if station and node:\n",
      "            self._table[station] = node\n",
      "\n",
      "  def size(self):\n",
      "    return len(self._table)\n",
      "\n",
      "  def find_node(self, station_name):\n",
      "    return self._table[station_name] if self._table.has_key(station_name) else None\n",
      "\n",
      "  def print_sample(self, N=10):\n",
      "    ks = random.sample(self._table, N)\n",
      "    for k in ks: print k, self._table[k]\n",
      "\n",
      "def sort_stations_by_lat(stations):\n",
      "  stations.sort(key=lambda x: x.lat)\n",
      "\n",
      "def sort_stations_by_lon(stations):\n",
      "  stations.sort(key=lambda x: x.lon)\n",
      "\n",
      "def find_weighted_median_index(stations):\n",
      "  total_weight = reduce(lambda sofar, item: sofar + item.weight, stations, 0)\n",
      "  half_weight = total_weight / 2\n",
      "\n",
      "  # binary search in acc array will be faster than linear search\n",
      "  acc = 0\n",
      "  for i in range(len(stations)):\n",
      "    acc += stations[i].weight\n",
      "    if (acc >= half_weight): break\n",
      "\n",
      "  return i\n",
      "\n",
      "\n",
      "# TODO: implement partition algo based on station weights\n",
      "# For now, use yoav's partitioned data structure.\n",
      "def partition(stations, direction='lat'):\n",
      "  if direction != 'lon': direction = 'lat'\n",
      "  if direction == 'lat': sort_stations_by_lat(stations)\n",
      "  else: sort_stations_by_lon(stations)\n",
      "  indx = find_weighted_median_index(stations)\n",
      "  return (indx, stations[i], stations[:i], stations[i+1:])\n",
      "\n",
      "class TreeNode:\n",
      "  def __init__(self):\n",
      "    self.parent = None\n",
      "    self.stations = []\n",
      "    self.children = []\n",
      "\n",
      "  def is_leaf(self):\n",
      "    return len(self.children) == 0\n",
      "\n",
      "def write_pickle(obj, fname):\n",
      "  with open(fname, 'wb') as f: pickle.dump(obj, f)\n",
      "\n",
      "def read_pickle(fname):\n",
      "  with open(fname, 'rb') as f: return pickle.load(f)\n",
      "\n",
      "def read_csv(fname):\n",
      "  with open(fname) as f:\n",
      "    return [Station(*line.strip().split(',')) for line in f.readlines()]\n",
      "\n",
      "def partition(stations):\n",
      "  root = TreeNode()\n",
      "  # build kd-tree from stations\n",
      "  return root\n",
      "\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "  pass\n",
      "  # stations = read_csv('station-lat-lon-weight.csv')\n",
      "  # sort_stations_by_lat(stations)\n",
      "  # print find_weighted_median(stations)\n",
      "\n",
      "  # station_to_node = StationToNodeTable()\n",
      "  # station_to_node.read_from_csv_file('station-to-node-table-yoav.csv')\n",
      "  # station_to_node.print_sample()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### mr_weather_concat_tminmax.py\n",
      "[back to top](#top-4) <a name='mr_weather_concat_tminmax'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load mr_weather_concat_tminmax.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "from mrjob.job import MRJob\n",
      "from sys import stderr\n",
      "from weather_data_parser import WeatherDataParser\n",
      "\n",
      "class MRConcatTminTmax(MRJob):\n",
      "\n",
      "  parser = None\n",
      "\n",
      "  def parse_line(self, line):\n",
      "    return MRConcatTminTmax.parser.parse_line(line)\n",
      "\n",
      "  def concat_tmin_tmax_mapper(self, _, line):\n",
      "    res = self.parse_line(line)\n",
      "\n",
      "    # stderr.write('line: ' + line + '\\n')\n",
      "    if res is not None:\n",
      "      station, year, measurement, vec, nr = res\n",
      "      if measurement == 'TMAX' or measurement == 'TMIN':\n",
      "        key = station + ':' + str(year)\n",
      "        # stderr.write('key: ' + key + '\\n')\n",
      "        stderr.write('key: ' + key + ' n: ' + str(nr) + '\\n')\n",
      "        # self.increment_counter('mapper', station)\n",
      "        yield key, (measurement, vec)\n",
      "\n",
      "  def concat_tmin_tmax_reducer(self, key, vals):\n",
      "    out = None\n",
      "    vals = list(vals)\n",
      "    # stderr.write('key: ' + key + ' vals: ' + str(vals) + '\\n')\n",
      "    for v in vals:\n",
      "      if out is None:\n",
      "        out = v[1]\n",
      "      else:\n",
      "        if v[0] == 'TMIN':\n",
      "          out = v[1] + out\n",
      "        else:\n",
      "          out = out + v[1]\n",
      "        stderr.write(key + ' is done.\\n')\n",
      "        yield key, out\n",
      "\n",
      "  def steps(self):\n",
      "    return [\n",
      "      self.mr(mapper=self.concat_tmin_tmax_mapper,\n",
      "              reducer=self.concat_tmin_tmax_reducer)\n",
      "    ]\n",
      "\n",
      "if __name__ == '__main__':\n",
      "  MRConcatTminTmax.parser = WeatherDataParser()\n",
      "  MRConcatTminTmax.run()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### run_mr_weather_concat_tminmax.sh\n",
      "[back to top](#top-4) <a name='run_mr_weather_concat_tminmax'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load run_mr_weather_concat_tminmax.sh"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#! /bin/sh\n",
      "python mr_concat_tmin_tmax.py -r emr --emr-job-flow-id j-32917857IT6TB \\\n",
      "  --setup 'export PYTHONPATH=$PYTHONPATH:weather_data_parser.tar.gz#/' \\\n",
      "  --output-dir s3://weather-analysis/weather-tminmax --no-output \\\n",
      "  s3://lge-bucket/weather-data/weather.csv\n",
      "\n",
      "# python mr_concat_tmin_tmax.py weather-tmin-tmax-tweak-10.csv\n",
      "# python mr_concat_tmin_tmax.py s3://lge-bucket/weather-data/weather-9424-of-9358395.csv\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### mr_weather_pca.py\n",
      "[back to top](#top-5) <a name='mr_weather_pca'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load mr_weather_pca.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sys import stderr\n",
      "from os import remove\n",
      "from os.path import basename\n",
      "import pickle\n",
      "from mrjob.job import MRJob\n",
      "from mrjob.protocol import JSONValueProtocol, RawValueProtocol, JSONProtocol, PickleProtocol\n",
      "import numpy as np\n",
      "from numpy.linalg import svd\n",
      "\n",
      "from geo_partition import StationToNodeTable\n",
      "\n",
      "def parse_natural_int(i):\n",
      "  try:\n",
      "    i = int(i)\n",
      "  except ValueError:\n",
      "    return 0\n",
      "  return i if i >= 0 else 0\n",
      "\n",
      "def get_node_ancestor_at_level(node, level):\n",
      "  level = parse_natural_int(level)\n",
      "  return node[:level]\n",
      "\n",
      "class MRPCA(MRJob):\n",
      "  INPUT_PROTOCOL = JSONProtocol\n",
      "  INTERNAL_PROTOCOL = PickleProtocol\n",
      "  OUTPUT_PROTOCOL = JSONValueProtocol\n",
      "\n",
      "  TYPES = {\n",
      "    'mu': 0,\n",
      "    'centered_vector': 1\n",
      "  }\n",
      "\n",
      "  def debug(self, obj):\n",
      "    if not self.options.no_debug: stderr.write(str(obj) + '\\n');\n",
      "\n",
      "  def _debug_options(self):\n",
      "    self.debug('--station-to-node: ')\n",
      "    self.debug(self.options.station_to_node)\n",
      "\n",
      "    self.debug('--no-debug: ')\n",
      "    self.debug(self.options.no_debug)\n",
      "\n",
      "    self.debug('--reduced-dimension: ')\n",
      "    self.debug(self.options.reduced_dimension)\n",
      "\n",
      "    self.debug('--explained-variance-ratio-threshold: ')\n",
      "    self.debug(self.options.explained_variance_ratio_threshold)\n",
      "\n",
      "    self.debug('--num-levels: ')\n",
      "    self.debug(self.options.num_levels)\n",
      "\n",
      "    self.debug('--store-mu: ')\n",
      "    self.debug(self.options.store_mu)\n",
      "\n",
      "    self.debug('--store-eigen-vectors: ')\n",
      "    self.debug(self.options.store_eigen_vectors)\n",
      "\n",
      "    self.debug('--station-to-node: ')\n",
      "    self.debug(self.options.station_to_node)\n",
      "\n",
      "  def node_mapper_init(self):\n",
      "    self._debug_options()\n",
      "    self.station_to_node_table = StationToNodeTable()\n",
      "    self.station_to_node_table.read_from_csv_file(basename(self.options.station_to_node))\n",
      "\n",
      "  def node_mapper(self, key, vec):\n",
      "    # self.debug('in mapper 1, key ' + key)\n",
      "\n",
      "    station, year = key.split(':')\n",
      "    leaf = self.station_to_node_table.find_node(station)\n",
      "\n",
      "    vec = np.array(vec)\n",
      "\n",
      "    # Yield vec to the corresponding leaf node and all of its\n",
      "    # ancestors.\n",
      "    level = self.options.num_levels\n",
      "    # level = MRPCA.NUM_OF_LEVELS\n",
      "    while level >= 0:\n",
      "      node = get_node_ancestor_at_level(leaf, level)\n",
      "      level -= 1\n",
      "      if self.options.reduced_dimension:\n",
      "        yield node, vec[:self.options.reduced_dimension]\n",
      "      else:\n",
      "        yield node, vec\n",
      "\n",
      "  def node_mean_reducer(self, node, vecs):\n",
      "    # self.debug('in mean reducer, node ' + node)\n",
      "\n",
      "    mu, n = None, 0\n",
      "    for vec in vecs:\n",
      "      if mu is None:\n",
      "        mu = vec.copy()\n",
      "      else:\n",
      "        mu += vec\n",
      "      n += 1\n",
      "    mu /= n\n",
      "\n",
      "  def node_substract_mean_reducer(self, node, vecs):\n",
      "    # self.debug('in reducer 1, node ' + node)\n",
      "\n",
      "    # I hope vecs can be hold in memory, but no :(\n",
      "    # It will complain memory problem.\n",
      "    # vecs = list(vecs)\n",
      "    # mu = np.mean(vecs)\n",
      "\n",
      "    # Solution is to write to local file first:\n",
      "    # Once mu is computed, read them back.\n",
      "    mu, n = None, 0\n",
      "    fname = 'vecs-' + node\n",
      "    with open(fname, 'w') as f:\n",
      "      for vec in vecs:\n",
      "        if mu is None:\n",
      "          mu = vec.copy()\n",
      "        else:\n",
      "          mu += vec\n",
      "        line = ','.join(map(str, vec.tolist()))\n",
      "        f.write(line + '\\n')\n",
      "        n += 1\n",
      "\n",
      "    mu /= float(n)\n",
      "\n",
      "    # Now we have mu, process the vecs:\n",
      "    with open(fname, 'r') as f:\n",
      "      for line in f:\n",
      "        vec = np.array(map(float, line.strip().split(',')))\n",
      "        vec -= mu\n",
      "        yield node, (MRPCA.TYPES['centered_vector'], vec)\n",
      "\n",
      "    if self.options.store_mu:\n",
      "      yield node, (MRPCA.TYPES['mu'], mu)\n",
      "\n",
      "    remove(fname)\n",
      "\n",
      "  def node_descriptor_reducer(self, node, vals):\n",
      "    # self.debug('in reducer 2, node ' + node)\n",
      "\n",
      "    cov = None\n",
      "    mu = None\n",
      "    nsamples = 0\n",
      "\n",
      "    for val in vals:\n",
      "      if val[0] == MRPCA.TYPES['mu']:\n",
      "        mu = val[1]\n",
      "      elif val[0] == MRPCA.TYPES['centered_vector']:\n",
      "        vec = val[1]\n",
      "        m = np.outer(vec, vec)\n",
      "        nsamples += 1\n",
      "        if cov is None:\n",
      "          cov = m\n",
      "        else:\n",
      "          cov += m\n",
      "    cov /= nsamples\n",
      "\n",
      "    ratio = self.options.explained_variance_ratio_threshold\n",
      "    U, D, V = svd(cov)\n",
      "    k, sofar, total = 1, 0.0, sum(D)\n",
      "    sofar += D[k-1]\n",
      "    while sofar < ratio * total:\n",
      "      k += 1\n",
      "      sofar += D[k-1]\n",
      "\n",
      "    desc_len = nsamples * k + (k + 1) * 730\n",
      "\n",
      "    # Dimension reduced descriptor for node.\n",
      "    # Only care about decriptor length\n",
      "    out = [node, k, nsamples, desc_len]\n",
      "\n",
      "    # Full descriptor, output might be huge.\n",
      "    # by default is not stored. But can be enabled.\n",
      "    if self.options.store_mu:\n",
      "      out.append(mu.tolist())\n",
      "\n",
      "    if self.options.store_eigen_vectors:\n",
      "      out.append(D[:k].tolist())\n",
      "      out.append(U[:, :k].tolist())\n",
      "\n",
      "    yield None, out\n",
      "\n",
      "  def steps(self):\n",
      "    return [\n",
      "      self.mr(mapper_init=self.node_mapper_init, mapper=self.node_mapper,\n",
      "              reducer=self.node_substract_mean_reducer),\n",
      "      self.mr(reducer=self.node_descriptor_reducer)\n",
      "    ]\n",
      "\n",
      "  def configure_options(self):\n",
      "    super(MRPCA, self).configure_options()\n",
      "\n",
      "    self.add_passthrough_option(\n",
      "      '--no-debug',\n",
      "      default=None,\n",
      "      dest='no_debug',\n",
      "      action='store_true',\n",
      "      help='Don\\'t output debug message, default is false.')\n",
      "\n",
      "    self.add_passthrough_option(\n",
      "      '--reduced-dimension',\n",
      "      type='int',\n",
      "      dest='reduced_dimension',\n",
      "      default=None,\n",
      "      help='Reduced vector dimension (full is 730) for debugging. If it is not'\n",
      "      'set, use full dimension. Default is not set.')\n",
      "\n",
      "    self.add_passthrough_option(\n",
      "      '--explained-variance-ratio-threshold',\n",
      "      type='float',\n",
      "      dest='explained_variance_ratio_threshold',\n",
      "      default=0.99,\n",
      "      help='The threshold of explaine variance ration used in PCA process. Default is 0.99.')\n",
      "\n",
      "    self.add_passthrough_option(\n",
      "      '--num-levels',\n",
      "      type='int',\n",
      "      dest='num_levels',\n",
      "      default=9,\n",
      "      help='Number of total partition levels. Default is 9.')\n",
      "\n",
      "    self.add_passthrough_option(\n",
      "      '--store-mu',\n",
      "      default=None,\n",
      "      dest='store_mu',\n",
      "      action='store_true',\n",
      "      help='Store mean vector. By default mean vector is not stored')\n",
      "\n",
      "    self.add_passthrough_option(\n",
      "      '--store-eigen-vectors',\n",
      "      default=None,\n",
      "      dest='store_eigen_vectors',\n",
      "      action='store_true',\n",
      "      help='Store eigen vectors. By default eigne vectors is not stored.')\n",
      "\n",
      "    self.add_file_option(\n",
      "      '--station-to-node',\n",
      "      default='data/station-to-node.csv',\n",
      "      help='The csv file describes the mapping between station-id to node-id.')\n",
      "\n",
      "if __name__ == '__main__':\n",
      "  MRPCA.run()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### python mr_weather_pca.py --help\n",
      "[back to top](#top-5) <a name='mr_weather_pca_usage'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python mr_weather_pca.py --help"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Usage: mr_weather_pca.py [options] [input files]\r\n",
        "\r\n",
        "Options:\r\n",
        "  --help                show this message and exit\r\n",
        "  --help-emr            show EMR-related options\r\n",
        "  --help-hadoop         show Hadoop-related options\r\n",
        "  --help-runner         show runner-related options\r\n",
        "  --no-debug            Don't output debug message, default is false.\r\n",
        "  --reduced-dimension=REDUCED_DIMENSION\r\n",
        "                        Reduced vector dimension (full is 730) for debugging. If it is notset, use\r\n",
        "                        full dimension. Default is not set.\r\n",
        "  --explained-variance-ratio-threshold=EXPLAINED_VARIANCE_RATIO_THRESHOLD\r\n",
        "                        The threshold of explaine variance ration used in PCA process. Default is\r\n",
        "                        0.99.\r\n",
        "  --num-levels=NUM_LEVELS\r\n",
        "                        Number of total partition levels. Default is 9.\r\n",
        "  --store-mu            Store mean vector. By default mean vector is not stored\r\n",
        "  --store-eigen-vectors\r\n",
        "                        Store eigen vectors. By default eigne vectors is not stored.\r\n",
        "  --station-to-node=STATION_TO_NODE\r\n",
        "                        The csv file describes the mapping between station-id to node-id.\r\n",
        "\r\n",
        "  Running specific parts of the job:\r\n",
        "    --mapper            run a mapper\r\n",
        "    --combiner          run a combiner\r\n",
        "    --reducer           run a reducer\r\n",
        "    --step-num=STEP_NUM\r\n",
        "                        which step to execute (default is 0)\r\n",
        "    --steps             print the mappers, combiners, and reducers that this job defines\r\n",
        "\r\n",
        "  Protocols:\r\n",
        "    --strict-protocols  If something violates an input/output protocol then raise an exception\r\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### run_mr_weather_pca.py\n",
      "[back to top](#top-5) <a name='run_mr_weather_pca'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load run_mr_weather_pca.sh"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#! /bin/sh\n",
      "JOB_FLOW_ID='j-XF54NKBXHHP0'\n",
      "\n",
      "MODE=$1\n",
      "if [[ $MODE == '' ]]; then\n",
      "  MODE=local\n",
      "fi\n",
      "\n",
      "if [[ $MODE == 'emr' ]]; then\n",
      "  python mr_weather_pca.py -r emr \\\n",
      "    --emr-job-flow-id ${JOB_FLOW_ID} \\\n",
      "    -c ~/.mrjob.conf \\\n",
      "    --explained-variance-ratio-threshold 0.99 \\\n",
      "    --station-to-node s3://weather-analysis/station-to-node-table-yoav.csv \\\n",
      "    --setup 'export PYTHONPATH=$PYTHONPATH:geo_partition.tar.gz#/' \\\n",
      "    --output-dir=s3://weather-analysis/node-descriptor-k-n-dl --no-output \\\n",
      "    --no-debug \\\n",
      "    s3://weather-analysis/weather-tminmax/\n",
      "elif [[ $MODE == 'emr-test' ]]; then\n",
      "  python mr_weather_pca.py -r emr \\\n",
      "    --emr-job-flow-id ${JOB_FLOW_ID} \\\n",
      "    -c ~/.mrjob.conf \\\n",
      "    --file s3://weather-analysis/station-to-node-table-yoav.csv \\\n",
      "    --setup 'export PYTHONPATH=$PYTHONPATH:geo_partition.tar.gz#/' \\\n",
      "    --output-dir=s3://weather-analysis/node-descriptor-k-n-dl --no-output \\\n",
      "    data/weather-tminmax-head-100.txt\n",
      "else\n",
      "  python mr_weather_pca.py \\\n",
      "    --reduced-dimension 5 \\\n",
      "    --explained-variance-ratio-threshold 0.99 \\\n",
      "    --store-mu \\\n",
      "    --store-eigen-vectors \\\n",
      "    --station-to-node data/station-to-node-table-yoav.csv \\\n",
      "    data/weather-tminmax-head-100.txt\n",
      "fi\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### partition_node_merge.py\n",
      "[back to top](#top-6) <a name='partition_node_merge'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load partition_node_merge.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Node:\n",
      "  def __init__(self, id, k, nsamples, desc_len, coord='', thres=0.0, should_merge=False):\n",
      "    self.id = id\n",
      "    self.k = int(k)\n",
      "    self.nsamples = int(nsamples)\n",
      "    self.desc_len = int(desc_len)\n",
      "    self.coord = str(coord)\n",
      "    self.thres = float(thres)\n",
      "    self.should_merge = bool(should_merge);\n",
      "\n",
      "  def __repr__(self):\n",
      "    return self.to_csv_line()\n",
      "\n",
      "  def level(self):\n",
      "    return len(self.id)\n",
      "\n",
      "  def left_child(self, nodes_dict):\n",
      "    id = self.id + '0'\n",
      "    return nodes_dict[id] if nodes_dict.has_key(id) else None\n",
      "\n",
      "  def right_child(self, nodes_dict):\n",
      "    id = self.id + '1'\n",
      "    return nodes_dict[id] if nodes_dict.has_key(id) else None\n",
      "\n",
      "  def parent(self, nodes_dict):\n",
      "    id = self.id[:-1]\n",
      "    return nodes_dict[id] if nodes_dict.has_key(id) else None\n",
      "\n",
      "  def to_csv_line(self):\n",
      "    l = map(str, [\n",
      "      self.id,\n",
      "      self.coord,\n",
      "      self.thres,\n",
      "      self.k,\n",
      "      self.nsamples,\n",
      "      self.desc_len,\n",
      "      '1' if self.should_merge else '0'\n",
      "    ])\n",
      "    return ','.join(l)\n",
      "\n",
      "def print_all_nodes(nodes_dict):\n",
      "  for k, v in nodes_dict.items():\n",
      "    print k, v\n",
      "\n",
      "def read_node_descriptors_from_csv(fname):\n",
      "  nodes_dict = {}\n",
      "  with open(fname) as f:\n",
      "    for line in f:\n",
      "      id, k, nsamples, desc_len = line.split(',')\n",
      "      nodes_dict[id] = Node(id, k, nsamples, desc_len)\n",
      "  return nodes_dict\n",
      "\n",
      "def join_node_spatial_info_from_csv(nodes_dict, fname):\n",
      "  with open(fname) as f:\n",
      "    for line in f:\n",
      "      id, coord, thres = line.split(',')\n",
      "      if nodes_dict.has_key(id):\n",
      "        nodes_dict[id].coord = coord\n",
      "        nodes_dict[id].thres = float(thres)\n",
      "  return nodes_dict\n",
      "\n",
      "def write_nodes_to_csv(nodes_dict, fname):\n",
      "  with open(fname, 'w') as f:\n",
      "    for _, n in nodes_dict.items():\n",
      "      f.write(n.to_csv_line())\n",
      "      f.write('\\n')\n",
      "\n",
      "def compute_whether_children_of_nodes_should_be_merged_at_level(nodes_dict, level):\n",
      "  nodes = [v for k, v in nodes_dict.items() if v.level() == level]\n",
      "\n",
      "  merge_count = 0\n",
      "  total_count = 0\n",
      "\n",
      "  for n in nodes:\n",
      "    total_count += 1\n",
      "    lc, rc = n.left_child(nodes_dict), n.right_child(nodes_dict)\n",
      "    if lc and rc and lc.desc_len + rc.desc_len > n.desc_len:\n",
      "      merge_count += 1\n",
      "      lc.should_merge = True\n",
      "      rc.should_merge = True\n",
      "  print level, merge_count, '/', total_count\n",
      "  return nodes_dict\n",
      "\n",
      "def compute_nodes_should_be_merged(nodes_dict, num_levels):\n",
      "  for i in range(num_levels + 1):\n",
      "    compute_whether_children_of_nodes_should_be_merged_at_level(nodes_dict, i)\n",
      "  return nodes_dict\n",
      "\n",
      "if __name__ == '__main__':\n",
      "  fin_name_1 = 'data/node-descriptor-k-n-dl-1-of-100.csv'\n",
      "  fin_name_2 = 'data/partition-tree-yoav.csv'\n",
      "  fout_name = 'data/partition-tree-nid-coord-thres-k-n-dl-m-1-of-100.csv'\n",
      "  num_levels = 9\n",
      "\n",
      "  nodes_dict = read_node_descriptors_from_csv(fin_name_1)\n",
      "  join_node_spatial_info_from_csv(nodes_dict, fin_name_2)\n",
      "  compute_nodes_should_be_merged(nodes_dict, num_levels)\n",
      "\n",
      "  write_nodes_to_csv(nodes_dict, fout_name)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}