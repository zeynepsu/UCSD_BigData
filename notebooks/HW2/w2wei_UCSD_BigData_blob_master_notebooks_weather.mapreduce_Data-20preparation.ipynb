{
 "metadata": {
  "name": "",
  "signature": "sha256:feaeb8137a6496f755fdb9ea634b55772d9b0f33e57663510e5461d8d747a5c0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<span style=\"color:blue\" > This section partitiions the stations accoding to effiective measurements (at least 50% coverage a year).</span>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import sklearn as sk\n",
      "import sys\n",
      "home_dir='/home/ubuntu/UCSD_BigData'\n",
      "dat_dir='/home/ubuntu/UCSD_BigData/data/weather'\n",
      "sys.path.append(home_dir+'/utils')\n",
      "from find_waiting_flow import *\n",
      "from AWS_keypair_management import *\n",
      "localData=home_dir+'/data/weather/ALL.head.csv'\n",
      "station=dat_dir+'/ghcnd-stations.txt'\n",
      "readme = dat_dir+'/ghcnd-readme.txt'\n",
      "import os\n",
      "sys.path.append('/usr/lib/python2.6/dist-packages')\n",
      "from mrjob.job import MRJob\n",
      "from sys import stderr\n",
      "import re,pickle,base64,zlib\n",
      "\n",
      "## AWS credentials\n",
      "Creds= pickle.load(open('/home/ubuntu/Vault/Creds.pkl','rb'))\n",
      "pair=Creds['mrjob']\n",
      "key_id=pair['key_id']\n",
      "secret_key=pair['secret_key']\n",
      "ID=pair['ID']\n",
      "job_flow_id=find_waiting_flow(key_id,secret_key)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile element_dist.py\n",
      "#!/usr/bin/python\n",
      "\"\"\"\n",
      "summarize the distribution of measurements\n",
      "\"\"\"\n",
      "import sys\n",
      "sys.path.append('/usr/lib/python2.6/dist-packages')\n",
      "from mrjob.job import MRJob\n",
      "import re\n",
      "from sys import stderr\n",
      "\n",
      "class measDist(MRJob):\n",
      "\n",
      "    def mapper(self, _, line):\n",
      "        try:\n",
      "            self.increment_counter('MrJob Counters','mapper-all',1)\n",
      "            elements=line.split(',')\n",
      "            if elements[0]=='station':\n",
      "                out=('header',1)\n",
      "            else:\n",
      "                out=(elements[1],1)\n",
      "        except Exception, e:\n",
      "            stderr.write('Error in line:\\n'+line)\n",
      "            stderr.write(e)\n",
      "            self.increment_counter('MrJob Counters','mapper-error',1)\n",
      "            out=('error',1)\n",
      "\n",
      "        finally:\n",
      "            yield out\n",
      "            \n",
      "    def combiner(self, word, counts):\n",
      "        self.increment_counter('MrJob Counters','combiner',1)\n",
      "        yield (word, sum(counts))\n",
      "\n",
      "    def reducer(self, word, counts):\n",
      "        self.increment_counter('MrJob Counters','reducer',1)\n",
      "        yield (word, sum(counts))\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    measDist.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting element_dist.py\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test\n",
      "# !python element_dist.py -r emr --emr-job-flow-id  $job_flow_id /home/ubuntu/UCSD_BigData/data/weather/ALL.head.csv > counts\n",
      "# The whole set\n",
      "# !python element_dist.py -r emr --emr-job-flow-id  $job_flow_id hdfs:/weather/weather.csv > counts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile tempDist.py\n",
      "## summarize all TMAX and TMIN measurements across stations\n",
      "#!/usr/bin/python\n",
      "\"\"\"\n",
      "summarize the distribution of tmax and tmin\n",
      "\"\"\"\n",
      "import sys\n",
      "sys.path.append('/usr/lib/python2.6/dist-packages')\n",
      "from mrjob.job import MRJob\n",
      "import re\n",
      "from sys import stderr\n",
      "\n",
      "class temp(MRJob):\n",
      "    def mapper(self, _, line):\n",
      "        try:\n",
      "            self.increment_counter('MrJob Counters','mapper-all',1)\n",
      "            elements=line.split(',')\n",
      "            if elements[1]=='TMAX':\n",
      "                out=('tmax',1)\n",
      "            elif elements[1]=='TMIN':\n",
      "                out=('tmin',1)\n",
      "        except Exception, e:\n",
      "            stderr.write('Error in line:\\n'+line)\n",
      "            stderr.write(e)\n",
      "            self.increment_counter('MrJob Counters','mapper-error',1)\n",
      "            out=('error',1)\n",
      "        finally:\n",
      "            yield out\n",
      "            \n",
      "    def combiner(self, word, counts):\n",
      "        self.increment_counter('MrJob Counters','combiner',1)\n",
      "        yield (word, sum(counts))\n",
      "\n",
      "    def reducer(self, word, counts):\n",
      "        self.increment_counter('MrJob Counters','reducer',1)\n",
      "        yield (word, sum(counts))\n",
      "if __name__ == '__main__':\n",
      "    temp.run()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting tempDist.py\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!python element_dist.py -r local /home/ubuntu/UCSD_BigData/data/weather/ALL.head.csv > temp_dist.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile sample.py\n",
      "# sample hdfs:/weather/weather.csv to make a test set.\n",
      "#!/usr/bin/python\n",
      "\"\"\"\n",
      "sample a small set for testing\n",
      "\"\"\"\n",
      "import random\n",
      "class sample(MRJob):\n",
      "    def mapper(self, _, line):\n",
      "        try:\n",
      "            self.increment_counter('MrJob Counters','mapper-all',1)\n",
      "            if random.random()<0.001: ## sample ratio 1/1000 here, I did 1/100 as well\n",
      "                yield (None,line)\n",
      "        except Exception, e:\n",
      "            stderr.write(str(e))\n",
      "            self.increment_counter('MrJob Counters','mapper-error',1)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    sample.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting sample.py\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# !python sample.py -r emr --emr-job-flow-id  $job_flow_id hdfs:/weather/weather.csv > subset_9k.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# clean the subset\n",
      "# dat = file(dat_dir+\"subset\").readlines()\n",
      "# fout=file(dat_dir+\"subset_9k\",\"wb\")\n",
      "# for d in dat:\n",
      "#     fout.write(d.split(\"\\t\")[1].split(\"\\n\")[0].strip('\\\"')+\"\\n\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile meas_count.py\n",
      "## count number of valid station/year. >50% TMAX and TMIN \n",
      "#!/usr/bin/python\n",
      "\"\"\"\n",
      "count the number of valid measurements. A valide record has at least 50% days with TMAX and TMIN.\n",
      "\"\"\"\n",
      "\n",
      "class countMeas(MRJob):\n",
      "    def mapper(self, _, line):\n",
      "        try:\n",
      "            self.increment_counter('MrJob Counters','mapper-all',1)\n",
      "            rec = line.split(\",\")\n",
      "            stn,meas,year = rec[:3]\n",
      "            stderr.write(stn+\"\\t\"+meas+\"\\t\"+year)\n",
      "            if meas=='TMAX' or meas=='TMIN': # this is measurement is about tmax of tmin\n",
      "                if len(filter(None,rec[3:]))/365.0>0.5: # measurements during >50% days of the year \n",
      "                    stderr.write(\"mapper: \"+\",\".join(rec[:3]))\n",
      "                    yield (stn,1)\n",
      "        except Exception, e:\n",
      "            stderr.write('Error in line:\\n'+str(line[:5]))\n",
      "            stderr.write(str(e))\n",
      "            self.increment_counter('MrJob Counters','mapper-error',1)\n",
      "\n",
      "    def reducer(self, key, value):\n",
      "        self.increment_counter('MrJob Counters','reducer',1)\n",
      "        try:\n",
      "            l_counts=[v for v in value]\n",
      "            s=sum(l_counts)\n",
      "            yield (key,s)\n",
      "\n",
      "        except Exception as e:\n",
      "            stderr.write('Error:')\n",
      "            stderr.write(str(e))\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    countMeas.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting meas_count.py\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# !python meas_count.py -r local /home/ubuntu/UCSD_BigData/data/weather/subset_9k  > meas_weight\n",
      "# !python meas_count.py -r emr --emr-job-flow-id  $job_flow_id hdfs:/weather/weather.csv  > meas_weight_all"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create or load a dataframe of station meta data\n",
      "try:\n",
      "    stationDF=pickle.load(file(os.path.join(dat_dir,\"stationDF.pkl\"))) # once stationDF.pkl generated, just load it\n",
      "except:\n",
      "    stnDat = file(os.path.join(dat_dir,\"ghcnd-stations.txt\"),'rb').readlines() # parse the raw data\n",
      "    stn = [re.split(\" +\", st)[:4] for st in stnDat]\n",
      "    ind = [l[0] for l in stn] # get the index\n",
      "    stn = [[st[0],float(st[1]),float(st[2]),float(st[3])] for st in stn]\n",
      "    stnDF = pd.DataFrame(data=stn,index=ind,columns=['ID','latitude','longitude','elevation'])\n",
      "    \n",
      "#     pickle.dump(stnDF,file(os.path.join(dat_dir,\"stationDF.pkl\"),\"wb\"))\n",
      "    # append a column of measurement weights to this metadata dataframe\n",
      "    measWT = file(\"meas_weight_all\").readlines()\n",
      "    measWT=[filter(None,re.split(\"[\\t\\n\\\"]\",rec)) for rec in measWT]\n",
      "    measWT=[[x[0],int(x[1])] for x in measWT]\n",
      "    ind = [m[0] for m in measWT]\n",
      "    measDF = pd.DataFrame(data=measWT,index=ind,columns=[\"ID\",\"weight\"])\n",
      "    del measDF[\"ID\"]\n",
      "    stationDF = stnDF.merge(measDF,how='outer',left_index=True,right_index=True) # merge the two dataframes\n",
      "    pickle.dump(stationDF,file(os.path.join(dat_dir,\"stationDF.pkl\"),\"wb\"))\n",
      "    \n",
      "finally:\n",
      "    print stationDF.ix[:10,]\n",
      "    testDF = stationDF[stationDF['weight']>0] # 27302 non NaN records"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                      ID  latitude  longitude  elevation  weight\n",
        "ACW00011604  ACW00011604   17.1167   -61.7833       10.1       2\n",
        "ACW00011647  ACW00011647   17.1333   -61.7833       19.2     NaN\n",
        "AE000041196  AE000041196   25.3330    55.5170       34.0      83\n",
        "AF000040930  AF000040930   35.3170    69.0170     3366.0       3\n",
        "AG000060390  AG000060390   36.7167     3.2500       24.0     146\n",
        "AG000060590  AG000060590   30.5667     2.8667      397.0     146\n",
        "AG000060611  AG000060611   28.0500     9.6331      561.0     108\n",
        "AG000060680  AG000060680   22.8000     5.4331     1362.0     146\n",
        "AGE00135039  AGE00135039   35.7297     0.6500       50.0     126\n",
        "AJ000037575  AJ000037575   41.5500    46.6670      490.0      71\n",
        "\n",
        "[10 rows x 5 columns]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## to build a tree. not yet completed. Function buildTree can reach every leaf node, sorting by latitude and longitude iteratively. \n",
      "test = testDF[:5]\n",
      "test['weight']=range(1,6)\n",
      "sortedTest = test.sort(columns=['latitude'])\n",
      "import numpy as np\n",
      "\n",
      "def cdf(df):\n",
      "    df['cdf']= np.zeros(len((df['weight'])))\n",
      "    df['cdf'][0]=df['weight'][0]\n",
      "    for i in xrange(1,len(df['weight'])):\n",
      "        df['cdf'][i]=df['cdf'][i-1]+df['weight'][i]\n",
      "    return df['cdf']\n",
      "# cdf.median = sortedTest['cdf'].median()\n",
      "\n",
      "def buildTree(df,latflg=1,it=1):\n",
      "#     print \"Iteration \",it\n",
      "    it+=1\n",
      "    if df.shape[0]<2:\n",
      "#         print \"leaf: \\n\",df\n",
      "        return df\n",
      "    else:\n",
      "        if latflg==1:\n",
      "            sortedDF = df.sort(columns=['latitude'])\n",
      "        else:\n",
      "            sortedDF = df.sort(columns=['longitude'])\n",
      "        latflg=-latflg # switch sorting direction\n",
      "        sortedDF['cdf']=cdf(sortedDF)#pd.Series(cdf(sortedDF['weight']),index=sortedDF.index)\n",
      "        mid = sortedDF['cdf'][-1]/2.0\n",
      "        sortedDFL = sortedDF[sortedDF['cdf']<=mid]\n",
      "        sortedDFR = sortedDF[sortedDF['cdf']>mid]\n",
      "#         print \"L \\n\",sortedDFL\n",
      "#         print \"R \\n\",sortedDFR\n",
      "        buildTree(sortedDFL,latflg,it)\n",
      "        buildTree(sortedDFR,latflg,it)\n",
      "buildTree(test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}