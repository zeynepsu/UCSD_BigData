{
 "metadata": {
  "name": "",
  "signature": "sha256:7cc2fd33e6ed1ff74ba5c70e57cfa7a23ea3cc207ed11ce199317e4c35458d06"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Part 2.b. Group together base-stations based on the state/prefecture/countries etc they belong to.\n",
      "Following the same general idea as we did in HWPart2.a we thought that it would be a better idea to use actual geospatial data to create the groups. While it holds that having a limited amount of data is not ideal when running PCA grouping together areas with completely different climatic characteristics might not be the best idea either!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Load libs!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import sklearn as sk\n",
      "print 'pandas version: ',pd.__version__\n",
      "print 'numpy version:',np.__version__\n",
      "print 'sklearn version:',sk.__version__"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "pandas version:  0.13.1\n",
        "numpy version: 1.8.1\n",
        "sklearn version: 0.14.1\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Using shapefiles\n",
      "\n",
      "We tried another way to partition the earth using the actual borders/shorelines which we got from this site http://www.gadm.org/ as shape files. These files are special because they use the WGS84, which is a standard World Geodetic System (it's the most recent and most widely used as well.) and most importantly is the same system that the weather.csv file uses to represent the location of the base-stations \n",
      "\n",
      "We also used this library https://code.google.com/p/pyshp/ to process the polygons included in the shapefiles.\n",
      "\n",
      "Finally, we used udig to visualize the shp files on our own machine. http://udig.refractions.net/\n",
      "\n",
      "\n",
      "shp2pgsql -s 26910 -W \"latin1\" GRC_adm2.shp greece gisdatabase > greece.sql\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Partition areas\n",
      "The code shown below creates a map that shows the smallest possible partitions that can be performed on the base-station data using the highest resolution shp file that represents the entire globe. Since this shapefile is quite big, the following code has been commented out as it would take hours to execute. Instead I am attaching a picture with the result generated after running this code for quite some time.\n",
      "\n",
      "If you wish to run this code make sure that you run it on a machine with enough memory, otherwise it will crush. To run it select the entire block of code and press ctrl+/ so that you can uncomment it evenly\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# # http://matplotlib.org/basemap/users/merc.html\n",
      "\n",
      "# from mpl_toolkits.basemap import Basemap\n",
      "# import numpy as np\n",
      "# import matplotlib.pyplot as plt\n",
      "# from matplotlib.collections import LineCollection\n",
      "# from matplotlib import cm\n",
      "# import shapefile\n",
      "\n",
      "\n",
      "# # llcrnrlat,llcrnrlon,urcrnrlat,urcrnrlon\n",
      "# # are the lat/lon values of the lower left and upper right corners\n",
      "# # of the map.\n",
      "# # lat_ts is the latitude of true scale.\n",
      "# # resolution = 'i' means use intermediate resolution coastlines.\n",
      "# lonmin=-180;lonmax=180;latsmin=-80;latsmax=80;\n",
      "# plt.figure(figsize=(15,10),dpi=300)\n",
      "# m = Basemap(projection='merc',llcrnrlat=latsmin,urcrnrlat=latsmax,\\\n",
      "#             llcrnrlon=lonmin,urcrnrlon=lonmax,lat_ts=20,resolution='i')\n",
      "# m.drawcoastlines()\n",
      "# m.fillcontinents(color='coral',lake_color='aqua')\n",
      "# ax = plt.subplot(111)\n",
      "# # draw parallels and meridians.\n",
      "# parallels = np.arange(-80,81,10.)\n",
      "# # labels = [left,right,top,bottom]\n",
      "# m.drawparallels(parallels,labels=[False,True,True,False])\n",
      "# meridians = np.arange(10.,351.,20.)\n",
      "# m.drawmeridians(meridians,labels=[True,False,False,True])\n",
      "\n",
      "# #m.drawparallels(np.arange(-90.,91.,30.))\n",
      "# #m.drawmeridians(np.arange(-180.,181.,60.))\n",
      "# m.drawmapboundary(fill_color='aqua')\n",
      "\n",
      "# r = shapefile.Reader(r\"/home/ubuntu/shapefiles/gadm2\")\n",
      "# shapes = r.shapes()\n",
      "# records = r.records()\n",
      "\n",
      "# for record, shape in zip(records,shapes):\n",
      "#     lons,lats = zip(*shape.points)\n",
      "#     data = np.array(m(lons, lats)).T\n",
      "\n",
      "#     if len(shape.parts) == 1:\n",
      "#         segs = [data,]\n",
      "#     else:\n",
      "#         segs = []\n",
      "#         for i in range(1,len(shape.parts)):\n",
      "#             index = shape.parts[i-1]\n",
      "#             index2 = shape.parts[i]\n",
      "#             segs.append(data[index:index2])\n",
      "#         segs.append(data[index2:])\n",
      "\n",
      "#     lines = LineCollection(segs,antialiaseds=(1,))\n",
      "#     lines.set_facecolors(cm.jet(np.random.rand(1)))\n",
      "#     lines.set_edgecolors('r')\n",
      "#     lines.set_linewidth(1)\n",
      "#     ax.add_collection(lines)\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "# plt.title('weather stations')\n",
      "# plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The red lines represent the edges. Conceptually when there are a lot of red lines it means that there is a smaller set of partitions at this area of the map.\n",
      "\n",
      "![Alt text](basemapfromshorelines.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's now use a smaller dataset..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from mpl_toolkits.basemap import Basemap\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.collections import LineCollection\n",
      "from matplotlib import cm\n",
      "import shapefile\n",
      "\n",
      "#r = shapefile.Reader(r\"/home/ubuntu/shapefiles/gadm2\")\n",
      "r = shapefile.Reader(r\"/home/ubuntu/shapefiles/Greece/GRC_adm2\")\n",
      "#r = shapefile.Reader(r\"/home/ubuntu/shapefiles/USA/USA_adm1\")\n",
      "#r = shapefile.Reader(r\"/home/ubuntu/shapefiles/globalv1/gadm1_lev0\")\n",
      "shapes = r.shapes()\n",
      "fields = r.fields\n",
      "#print fields\n",
      "\n",
      "AllGeoms = []\n",
      "records = r.records()\n",
      "#shape(shapes)\n",
      "print len(zip(records,shapes)), 'polygons loaded!'\n",
      "for record, shapez in zip(records,shapes):\n",
      "    lons,lats = zip(*shapez.points)\n",
      "    #print np.shape(lons), np.shape(lats)\n",
      "    \n",
      "    kk = zip(lats,lons)\n",
      "    AllGeoms.append(kk)\n",
      "    #print np.shape(kk)\n",
      "    \n",
      "    #print kk\n",
      "    #data = np.array(m(lons, lats)).T\n",
      "#     data = np.array(lons, lats).T\n",
      "    \n",
      "#     print data\n",
      "#     if len(shape.parts) == 1:\n",
      "#         segs = [data,]\n",
      "#     else:\n",
      "#         segs = []\n",
      "#         for i in range(1,len(shape.parts)):\n",
      "#             index = shape.parts[i-1]\n",
      "#             index2 = shape.parts[i]\n",
      "#             segs.append(data[index:index2])\n",
      "#         segs.append(data[index2:])\n",
      "\n",
      "#     lines = LineCollection(segs,antialiaseds=(1,))\n",
      "#     lines.set_facecolors(cm.jet(np.random.rand(1)))\n",
      "#     lines.set_edgecolors('r')\n",
      "#     lines.set_linewidth(1)\n",
      "#     ax.add_collection(lines)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "52 polygons loaded!\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to find out if a station belongs to an area we need to see if a point belongs to a convex hull. This is what the following function does."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def in_hull(p, hull):\n",
      "    \"\"\"\n",
      "    Test if points in `p` are in `hull`\n",
      "\n",
      "    `p` should be a `NxK` coordinates of `N` points in `K` dimension\n",
      "    `hull` is either a scipy.spatial.Delaunay object or the `MxK` array of the \n",
      "    coordinates of `M` points in `K`dimension for which a Delaunay triangulation\n",
      "    will be computed\n",
      "    \"\"\"\n",
      "    from scipy.spatial import Delaunay\n",
      "    if not isinstance(hull,Delaunay):\n",
      "        hull = Delaunay(hull)\n",
      "\n",
      "    return hull.find_simplex(p)>=0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's test it!\n",
      "\n",
      "Y = [[35.435096, 23.768364]] ## That's in Greece\n",
      "#Y = [[32.975025, -116.861099]]   ## That's in California\n",
      "hullCounter=0\n",
      "result = False\n",
      "for hull in AllGeoms:\n",
      "\n",
      "    result = in_hull(Y,hull)\n",
      "    #print result\n",
      "    if result==True:\n",
      "        \n",
      "        print hullCounter\n",
      "        break\n",
      "    hullCounter+=1\n",
      "    \n",
      "#plot_in_hull(Y,X)\n",
      "if result==False:\n",
      "    print 'Nope! The selected point doesn\\'t belong to the selected dataset'\n",
      "else:\n",
      "    print 'Yay! ', Y,' belongs to our dataset'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "30\n",
        "Yay!  [[35.435096, 23.768364]]  belongs to our dataset\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Let's load the data from the csv file we created on the last part..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "header_row=['station','maxcount','mincount'] # note the header has duplicate column values\n",
      "maxmindf = pd.read_csv('basestationmaxmin.csv', names=header_row)\n",
      "maxmindf.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>station</th>\n",
        "      <th>maxcount</th>\n",
        "      <th>mincount</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> AG000060390</td>\n",
        "      <td> 365</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> AG000060611</td>\n",
        "      <td> 730</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> AG000060680</td>\n",
        "      <td> 365</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> AGE00135039</td>\n",
        "      <td>  26</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> AJ000037575</td>\n",
        "      <td> 209</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 3 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "       station  maxcount  mincount\n",
        "0  AG000060390       365         0\n",
        "1  AG000060611       730         0\n",
        "2  AG000060680       365         0\n",
        "3  AGE00135039        26         0\n",
        "4  AJ000037575       209         0\n",
        "\n",
        "[5 rows x 3 columns]"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!gunzip stations.pkl.gz\n",
      "import pickle\n",
      "!gunzip -c stations.pkl.gz > stations.pkl  # This command also keeps the initial gz file....\n",
      "stations=pickle.load(open('stations.pkl', 'rb'))\n",
      "df = pd.read_pickle('stations.pkl')\n",
      "df['station'] = df.index # let's create a column with the indexes (base-station names)\n",
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>latitude</th>\n",
        "      <th>longitude</th>\n",
        "      <th>elevation</th>\n",
        "      <th>state</th>\n",
        "      <th>name</th>\n",
        "      <th>GSNFLAG</th>\n",
        "      <th>HCNFLAG</th>\n",
        "      <th>WMOID</th>\n",
        "      <th>station</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>ACW00011604</th>\n",
        "      <td> 17.1167</td>\n",
        "      <td>-61.7833</td>\n",
        "      <td>   10.1</td>\n",
        "      <td> NaN</td>\n",
        "      <td> ST JOHNS COOLIDGE FLD</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td> ACW00011604</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>ACW00011647</th>\n",
        "      <td> 17.1333</td>\n",
        "      <td>-61.7833</td>\n",
        "      <td>   19.2</td>\n",
        "      <td> NaN</td>\n",
        "      <td>              ST JOHNS</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td> ACW00011647</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>AE000041196</th>\n",
        "      <td> 25.3330</td>\n",
        "      <td> 55.5170</td>\n",
        "      <td>   34.0</td>\n",
        "      <td> NaN</td>\n",
        "      <td>   SHARJAH INTER. AIRP</td>\n",
        "      <td> GSN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> 41196</td>\n",
        "      <td> AE000041196</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>AF000040930</th>\n",
        "      <td> 35.3170</td>\n",
        "      <td> 69.0170</td>\n",
        "      <td> 3366.0</td>\n",
        "      <td> NaN</td>\n",
        "      <td>          NORTH-SALANG</td>\n",
        "      <td> GSN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> 40930</td>\n",
        "      <td> AF000040930</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>AG000060390</th>\n",
        "      <td> 36.7167</td>\n",
        "      <td>  3.2500</td>\n",
        "      <td>   24.0</td>\n",
        "      <td> NaN</td>\n",
        "      <td>    ALGER-DAR EL BEIDA</td>\n",
        "      <td> GSN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> 60390</td>\n",
        "      <td> AG000060390</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 9 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "             latitude  longitude  elevation state                   name  \\\n",
        "ACW00011604   17.1167   -61.7833       10.1   NaN  ST JOHNS COOLIDGE FLD   \n",
        "ACW00011647   17.1333   -61.7833       19.2   NaN               ST JOHNS   \n",
        "AE000041196   25.3330    55.5170       34.0   NaN    SHARJAH INTER. AIRP   \n",
        "AF000040930   35.3170    69.0170     3366.0   NaN           NORTH-SALANG   \n",
        "AG000060390   36.7167     3.2500       24.0   NaN     ALGER-DAR EL BEIDA   \n",
        "\n",
        "            GSNFLAG HCNFLAG  WMOID      station  \n",
        "ACW00011604     NaN     NaN    NaN  ACW00011604  \n",
        "ACW00011647     NaN     NaN    NaN  ACW00011647  \n",
        "AE000041196     GSN     NaN  41196  AE000041196  \n",
        "AF000040930     GSN     NaN  40930  AF000040930  \n",
        "AG000060390     GSN     NaN  60390  AG000060390  \n",
        "\n",
        "[5 rows x 9 columns]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = pd.merge(df, maxmindf, on='station', how='inner')\n",
      "res.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>latitude</th>\n",
        "      <th>longitude</th>\n",
        "      <th>elevation</th>\n",
        "      <th>state</th>\n",
        "      <th>name</th>\n",
        "      <th>GSNFLAG</th>\n",
        "      <th>HCNFLAG</th>\n",
        "      <th>WMOID</th>\n",
        "      <th>station</th>\n",
        "      <th>maxcount</th>\n",
        "      <th>mincount</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 36.7167</td>\n",
        "      <td>  3.2500</td>\n",
        "      <td>   24</td>\n",
        "      <td> NaN</td>\n",
        "      <td>     ALGER-DAR EL BEIDA</td>\n",
        "      <td> GSN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> 60390</td>\n",
        "      <td> AG000060390</td>\n",
        "      <td> 365</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 28.0500</td>\n",
        "      <td>  9.6331</td>\n",
        "      <td>  561</td>\n",
        "      <td> NaN</td>\n",
        "      <td>              IN-AMENAS</td>\n",
        "      <td> GSN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> 60611</td>\n",
        "      <td> AG000060611</td>\n",
        "      <td> 730</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 22.8000</td>\n",
        "      <td>  5.4331</td>\n",
        "      <td> 1362</td>\n",
        "      <td> NaN</td>\n",
        "      <td>            TAMANRASSET</td>\n",
        "      <td> GSN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> 60680</td>\n",
        "      <td> AG000060680</td>\n",
        "      <td> 365</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 35.7297</td>\n",
        "      <td>  0.6500</td>\n",
        "      <td>   50</td>\n",
        "      <td> NaN</td>\n",
        "      <td> ORAN-HOPITAL MILITAIRE</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td> AGE00135039</td>\n",
        "      <td>  26</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 41.5500</td>\n",
        "      <td> 46.6670</td>\n",
        "      <td>  490</td>\n",
        "      <td> NaN</td>\n",
        "      <td>               ZAKATALA</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> 37575</td>\n",
        "      <td> AJ000037575</td>\n",
        "      <td> 209</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 11 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "   latitude  longitude  elevation state                    name GSNFLAG  \\\n",
        "0   36.7167     3.2500         24   NaN      ALGER-DAR EL BEIDA     GSN   \n",
        "1   28.0500     9.6331        561   NaN               IN-AMENAS     GSN   \n",
        "2   22.8000     5.4331       1362   NaN             TAMANRASSET     GSN   \n",
        "3   35.7297     0.6500         50   NaN  ORAN-HOPITAL MILITAIRE     NaN   \n",
        "4   41.5500    46.6670        490   NaN                ZAKATALA     NaN   \n",
        "\n",
        "  HCNFLAG  WMOID      station  maxcount  mincount  \n",
        "0     NaN  60390  AG000060390       365         0  \n",
        "1     NaN  60611  AG000060611       730         0  \n",
        "2     NaN  60680  AG000060680       365         0  \n",
        "3     NaN    NaN  AGE00135039        26         0  \n",
        "4     NaN  37575  AJ000037575       209         0  \n",
        "\n",
        "[5 rows x 11 columns]"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print df[['latitude']].tolist()\n",
      "bsCoords = zip(res[\"latitude\"].tolist(),df[\"longitude\"].tolist())\n",
      "stationNames = res[\"station\"].tolist()\n",
      "#bslongs = df[\"longitude\"].tolist()\n",
      "#print bsCoords"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# for hull in AllGeoms:\n",
      "#     ftz = in_hull(bsCoords,kk)\n",
      "\n",
      "\n",
      "# indices = []\n",
      "# cntr = 0\n",
      "# for ft in ftz:\n",
      "#     if ft==True:\n",
      "#         print 'found!'\n",
      "#         indices.append(cntr)\n",
      "#     cntr+=1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print indices # Uncomment this to find the indices of the stations that belong to the geometries"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Uncomment this to find out what the station name and the coordinates of a basestation is based on the indec above\n",
      "## sample output:\n",
      "## [(39.649999999999999, 22.449999999999999)]\n",
      "## GR000016648\n",
      "\n",
      "\n",
      "# print [bsCoords[29423]]\n",
      "# print stationNames[29423]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## This prints the names of the fields that each shp file has\n",
      "print fields"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('DeletionFlag', 'C', 1, 0), ['ID_0', 'N', 9, 0], ['ISO', 'C', 3, 0], ['NAME_0', 'C', 75, 0], ['ID_1', 'N', 9, 0], ['NAME_1', 'C', 75, 0], ['ID_2', 'N', 9, 0], ['NAME_2', 'C', 75, 0], ['NL_NAME_2', 'C', 75, 0], ['VARNAME_2', 'C', 150, 0], ['TYPE_2', 'C', 50, 0], ['ENGTYPE_2', 'C', 50, 0]]\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Each shape file has the usefull information in different parts.\n",
      "\n",
      "The following code: </br>\n",
      "\n",
      "print records[96][28], records[96][3], records[96][29]\n",
      "\n",
      "has output: Southern Europe Greece Europe for one of the datasets.\n",
      "\n",
      "\n",
      "This is the kind of information we will need for our iteration. We will first compute the PCA for the stations that belong to Greece later we will do the same for the ones that belong in Southern Europe we will use the relation shown here: http://seed.ucsd.edu/mediawiki/index.php/BigDataHW2#Suggested_Steps and if the need for a better grouping appears we will do the same for the points that belong in Europe."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "# Country = records[84107][2]\n",
      "# # for i in range(len(records[84151])):\n",
      "# #     print records[84151][i],'---',i\n",
      "    \n",
      "# print records[84107][3], records[84107][5]\n",
      "# print records[84151][3], records[84151][5]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print records[96][28], records[96][3], records[96][29]\n",
      "\n",
      "# For the GRC_adm2 dataset:\n",
      "print 'Greater region: ',records[2][2],', medium region: ', records[2][4], ', most specific region: ',records[2][6]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Greater region:  Greece , medium region:  Anatoliki Makedonia kai Thraki , most specific region:  Kavala\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Dead-end!\n",
      "\n",
      "The following code is supposed to create the mappings between the basestations and the regions they belong to. Unfortunately it is terribly inefficient so it doesn't perform well for huge datasets."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "\n",
      "import collections\n",
      "d1 = collections.defaultdict(list)\n",
      "d2 = collections.defaultdict(list)\n",
      "d3 = collections.defaultdict(list)\n",
      "for jj in range(len(bsCoords)):\n",
      "    Y = [bsCoords[jj]]\n",
      "    hullCounter=0\n",
      "    for hull in AllGeoms:\n",
      "\n",
      "        result = in_hull(Y,hull)\n",
      "        #print result\n",
      "        if result==True:\n",
      "            general = records[hullCounter][4] \n",
      "            specific = records[hullCounter][6]\n",
      "            moregeneral = records[hullCounter][2] \n",
      "            d1[stationNames[jj]].append(specific)\n",
      "            d2[stationNames[jj]].append(general)\n",
      "            d3[stationNames[jj]].append(moregeneral)\n",
      "            #print hullCounter\n",
      "            break\n",
      "        hullCounter+=1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-30-360ca0389577>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mhull\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mAllGeoms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0min_hull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhull\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;31m#print result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-2-ecd4a16e3798>\u001b[0m in \u001b[0;36min_hull\u001b[1;34m(p, hull)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspatial\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDelaunay\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhull\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mDelaunay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mhull\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDelaunay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhull\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mhull\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_simplex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/scipy/spatial/qhull.so\u001b[0m in \u001b[0;36mscipy.spatial.qhull.Delaunay.__init__ (scipy/spatial/qhull.c:14197)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/scipy/spatial/qhull.so\u001b[0m in \u001b[0;36mscipy.spatial.qhull._QhullUser.__init__ (scipy/spatial/qhull.c:12816)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/scipy/spatial/qhull.so\u001b[0m in \u001b[0;36mscipy.spatial.qhull.Delaunay._update (scipy/spatial/qhull.c:14575)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/scipy/spatial/qhull.so\u001b[0m in \u001b[0;36mscipy.spatial.qhull._QhullUser._update (scipy/spatial/qhull.c:13306)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/numpy/core/_methods.pyc\u001b[0m in \u001b[0;36m_amin\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m     17\u001b[0m                             out=out, keepdims=keepdims)\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0m_amin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     return um.minimum.reduce(a, axis=axis,\n\u001b[0;32m     21\u001b[0m                             out=out, keepdims=keepdims)\n",
        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "with open('geogrouping.json', 'w') as outfile:\n",
      "  json.dump(d, outfile)\n",
      "    \n",
      "import pickle\n",
      "with open('geogrouping.pkl', 'w') as outfile:\n",
      "  pickle.dump(d, outfile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#need to check if the file is infact correct... \n",
      "!head -c 300 geogrouping.json"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This code kept running for hours without concluding to a useful grouping. Instead we decided to use a more efficient approach! The problem here was clearly the fact that we weren't able to find out if a point was inside a convex hull in an efficient manner. So we decided to setup postgres and postgis in order to be able to effectively query the results.\n",
      "\n",
      "In case someone else decides to try the same approach, I am going to save you some time. Use [this guide](http://trac.osgeo.org/postgis/wiki/UsersWikiPostGIS20Ubuntu1204 \"Title\") to set up the database and the environment it is by far the most useful guide out there. \n",
      "\n",
      "We used shp2pgsql to generate the sql file from the shp one but we soon found out that there are compatibility issues that are probably not going to be solved before the deadline...\n",
      "\n",
      "I will keep working on it after the due date even if I don't get credit for it... :)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Great let's build the map/reduce job that computes the PCA for each group!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile pcatogroups.py\n",
      "#!/usr/bin/python\n",
      "\"\"\"\n",
      "count the number of measurements for each base-station\n",
      "\"\"\"\n",
      "import sys\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import sklearn as sk\n",
      "sys.path.append('/usr/lib/python2.6/dist-packages')\n",
      "from mrjob.job import MRJob\n",
      "from mrjob.step import MRStep\n",
      "import re\n",
      "from sys import stderr\n",
      "import json\n",
      "\n",
      "class SqliteJob(MRJob):\n",
      "    \n",
      "    def __init__(self, *args, **kwargs):\n",
      "        super(SqliteJob, self).__init__(*args, **kwargs)\n",
      "        self.data = {}\n",
      "        \n",
      "    \n",
      "\n",
      "    def configure_options(self):\n",
      "        super(SqliteJob, self).configure_options()\n",
      "        self.add_file_option('--hashmap')\n",
      "\n",
      "    def mapper_init(self):\n",
      "        json_data=open(self.options.hashmap)\n",
      "\n",
      "        self.data = json.load(json_data)\n",
      "        \n",
      "        #sys.stderr.write(str(data))\n",
      "        json_data.close()\n",
      "        # make dictionary available to mapper\n",
      "        #sys.stderr.write('MAPPER_INIT!!')\n",
      "        sys.stderr.write(str(self.options.hashmap))\n",
      "        \n",
      "        #self.sqlite_conn = sqlite3.connect(self.options.database)\n",
      "    def mapper(self, _, line):\n",
      "#         for key in self.data:\n",
      "#             sys.stderr.write( key + ' corresponds to'+ str(self.data[key])+'\\n')\n",
      "\n",
      "        try:\n",
      "            self.increment_counter('MrJob Counters','mapper-all',1)\n",
      "            elements=line.split(',')\n",
      "            if elements[0]=='station':\n",
      "                \n",
      "                # if the header is spotted add it, it's not going to matter\n",
      "                # since no base-station with name: 'header' exists, so when we do the\n",
      "                # inner join this will be pruned\n",
      "                \n",
      "                out=('header','0~0')\n",
      "                yield out\n",
      "            else:\n",
      "                if elements[1]=='TMAX':\n",
      "                    \n",
      "                    # Uncomment that to get measurements from a specific year.\n",
      "                    #if elements[2]=='1973':\n",
      "                        \n",
      "                        # Use the following instead if you want to avoid null values\n",
      "                        #out=(str(self.data[elements[0]]),str(filter(None,elements[3:])))#+'~'+str(0))\n",
      "                        \n",
      "                        out=(str(self.data[elements[0]]),(elements[3:]))#+'~'+str(0))\n",
      "                        yield out\n",
      "                #elif elements[1]=='TMIN':\n",
      "                    \n",
      "                    # Uncomment that to get measurements from a specific year.\n",
      "                    #if elements[2]=='1973':\n",
      "                        \n",
      "               #         out=(elements[0],str(0)+'~'+str(len(filter(None, elements[3:]))))\n",
      "               #         yield out\n",
      "        except Exception, e:\n",
      "            #stderr.write('Error in line:\\n'+line)\n",
      "            #stderr.write(e)\n",
      "            self.increment_counter('MrJob Counters','mapper-error',1)\n",
      "            \n",
      "            # Again... This will be pruned when we do the inner join later...\n",
      "            \n",
      "            out=('error','0~0')\n",
      "            yield out\n",
      "\n",
      "        \n",
      "#     def combiner(self, station, vals):\n",
      "#         yield (0,0)\n",
      "        \n",
      "    def reducer(self, group, vals):\n",
      "\n",
      "        self.increment_counter('MrJob Counters','reducer',1)\n",
      "        sys.stderr.write('group range is: '+group+'\\n')\n",
      "        valArr = []\n",
      "        \n",
      "        for val in vals:\n",
      "            #sys.stderr.write(str(val)+'\\n')\n",
      "            valArr.append(val)\n",
      "\n",
      "\n",
      "\n",
      "        # Awesome! Let's use a Dataframe!\n",
      "        ddtf = pd.DataFrame(valArr,columns = range(1,366))\n",
      "        \n",
      "        # Since we have a lot of empty values replace them with np.nan \n",
      "        # this is gonna make the processing easier\n",
      "        \n",
      "        ddtf = ddtf.replace('', np.nan)\n",
      "        \n",
      "        # Let's check out the measurements\n",
      "        #sys.stderr.write(str(ddtf.head()))  # Checked!\n",
      "        \n",
      "\n",
      "        \n",
      "        # switch the type of every element in the dataframe from an object to a float\n",
      "        for listElem in list(ddtf.columns.values):\n",
      "            #sys.stderr.write(str(listElem))\n",
      "            ddtf[listElem] = ddtf[listElem].astype(float)#.fillna(0.0)\n",
      "        \n",
      "        \n",
      "        # It should be float now\n",
      "        # sys.stderr.write('\\nddtf types: '+str(ddtf.dtypes))   # Yes it is!\n",
      "\n",
      "        \n",
      "        M=ddtf.loc[:,:].transpose()\n",
      "        \n",
      "        # check the shape of M\n",
      "        #sys.stderr.write('\\nM shape is: '+str(M.shape))\n",
      "        \n",
      "        M=M.dropna(axis=1)\n",
      "        \n",
      "        # check again after dropping the nan values\n",
      "        sys.stderr.write('\\n M shape after droping NaNs is: '+str(np.shape(M)))\n",
      "        \n",
      "        (columns,rows)=np.shape(M)\n",
      "        \n",
      "        # check if there are any values in this group\n",
      "        \n",
      "        \n",
      "        # Let's compute the covariance, (We could use np.cov instead...)\n",
      "        if rows !=0:\n",
      "            Mean=np.mean(M, axis=1).values\n",
      "\n",
      "\n",
      "            C=np.zeros([columns,columns])   # Sum\n",
      "            N=np.zeros([columns,columns])   # Counter of non-nan entries\n",
      "\n",
      "            for i in range(rows):\n",
      "                row=M.iloc[:,i]-Mean;\n",
      "                outer=np.outer(row,row)\n",
      "                valid=np.isnan(outer)==False\n",
      "                C[valid]=C[valid]+outer[valid]  # update C with the valid location in outer\n",
      "                N[valid]=N[valid]+1\n",
      "            valid_outer=np.multiply(1-np.isnan(N),N>0)\n",
      "            cov=np.divide(C,N)\n",
      "            cov2 = np.cov(M)\n",
      "            sys.stderr.write('\\ncov is: \\n'+str(cov)+'\\ncov2 is: \\n'+str(cov2))\n",
      "#             for elem in cov:\n",
      "#                 sys.stderr.write('\\ncov is: \\n'+str(elem))\n",
      "            sys.stderr.write('\\nnp.sum(cov) : '+str(np.sum(cov)))\n",
      "            summation = np.sum(cov)\n",
      "            if summation!=0:\n",
      "                U,D,V=np.linalg.svd(cov)\n",
      "                sys.stderr.write('\\nyielding D')\n",
      "                yield (group+str(rows),str(D))\n",
      "\n",
      "                \n",
      "        else:\n",
      "            sys.stderr.write('nothing to do here')\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    SqliteJob.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Grade ###\n",
      "\n",
      "It is clear that you put a lot of work into this, but much of it is misguided:\n",
      "\n",
      "1. Why does the partitioning into cubes of longitude/latitude make sense?\n",
      "2. Why does splitting according to county borders make sense (I appreciate that it is hard)\n",
      "3. Why yse 99% as the threshold when selecting number of eigen-vectors?\n",
      "\n",
      "You wrote *some* text between the code cells, but these are mostly titles or explanations that\n",
      "something does not work. There are not enough explanations of what you are doing or of the figures you are generating.\n",
      "\n",
      "Grade: 80"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}