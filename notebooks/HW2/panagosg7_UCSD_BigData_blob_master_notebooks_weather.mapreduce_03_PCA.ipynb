{
 "metadata": {
  "name": "",
  "signature": "sha256:f28c792ae13c99a11385ab8ec72c2cae2c9abd440943bc87ec6dd07ab1e8531e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Principal Component Analysis (MapReduce)\n",
      "\n",
      "This module does Principal Component Analysis (PCA) based on Singular Value \n",
      "Decomposition (SVD) on MapReduce.\n",
      "\n",
      "**DISCLAIMER**: I was planning to use the combined data of `TMAX` and `TMIN` for each group but \n",
      "didn't have enough time. So instead, in the rest of the analysis we are limiting ouselves to just \n",
      "`TMAX`. Extending the value-vectors to include `TMIN` as well should not be very trikcy. The length \n",
      "for the vectors in that case would be $365\\cdot 2$ instead of $365$. One difficulty would be to\n",
      "bunch up the measurements that have the same `STATION_ID` and `YEAR`, and `TYPE` $\\in$\n",
      "{`TMAX`,`TMIN`} into a single line before passing it forward to the next step of the map-reduce. This\n",
      "could be done as a separete initial step, before the PCA analysis is run. **END OF DISCLAIMER**\n",
      "\n",
      "\n",
      "Inputs to this module are:\n",
      "* **Stations**: a file containing information about stations include the group to which the station belongs\n",
      "* **Groups** of interest. We might need to perform PCA on each group in the world but just specific ones\n",
      "* **weather.csv**: file containing weather data about various measurements. Each line has the station id,\n",
      "    the type of measurement, the date and the values for each one of the 365 days of the year.\n",
      "\n",
      "The **mapper** pairs up lines of the original file that belong to the same group and refer to the \n",
      "measurement of interest. Here this is `TMAX`.\n",
      "\n",
      "The **combiner** has not bee overriden.\n",
      "\n",
      "The **reducer** takes as input lines from the input file that correspond to the same group and performs \n",
      "the same calculation (PCA through SVD) we performed for Homework 1. The details of this method can be found\n",
      "here:\n",
      "\n",
      "http://nbviewer.ipython.org/github/panagosg7/UCSD_BigData/blob/HW1v2r2/notebooks/weather/Weather%20Analysis.ipynb\n",
      "\n",
      "\n",
      "The **output** of each reduce step of this analysis is a line that contains the group id and a series of \n",
      "$365$ values. The $i^{th}$ of these values corresponds to the variance explained by the first $i$ eigenvectors.\n",
      "\n",
      "\n",
      "On some later iteration we could add support for returning the eigenvectors and eigenvalues as well so that \n",
      "we can perform recostruction of missing values (for example). There is code in comments that dumps the \n",
      "eigenvectors for each group in a folder in S3, but this feature is disabled at the moment."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile pca.py\n",
      "#!/usr/bin/python\n",
      "\n",
      "\"\"\"\n",
      "<description here>\n",
      "\"\"\"\n",
      "\n",
      "import sys\n",
      "sys.path.append('/usr/lib/python2.6/dist-packages')\n",
      "from mrjob.job import MRJob\n",
      "import re\n",
      "from sys import stderr\n",
      "import gzip\n",
      "import pickle\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "class MRPCA(MRJob):\n",
      "\n",
      "    def configure_options(self):\n",
      "        super(MRPCA,self).configure_options()\n",
      "        # stations file\n",
      "        self.add_file_option('--stations')\n",
      "        \n",
      "        # groups to take into account\n",
      "        # this will automatically compute \n",
      "        self.add_passthrough_option('--groups', \n",
      "                                    type='string', \n",
      "                                    default='',\n",
      "                                    help='groups to take into account in comma separated form')\n",
      "        \n",
      "        self.add_passthrough_option('--key', type='string', default='')        \n",
      "        self.add_passthrough_option('--secret', type='string', default='')\n",
      "        \n",
      "     \n",
      "    def parse_cs_ints(self,s):\n",
      "         for s in s.split(','):\n",
      "            try:\n",
      "                yield int(s)\n",
      "            except Exception as e:\n",
      "                pass               \n",
      "\n",
      "\n",
      "        \n",
      "    def mapper_init(self):\n",
      "        f = gzip.open( self.options.stations, \"rb\" )\n",
      "        pickleFile = pickle.Unpickler( f )\n",
      "        self.stations = pickleFile.load()[['latitude','longitude','group_id']]\n",
      "        f.close()\n",
      "            \n",
      "        # valid groups\n",
      "        if (self.options.groups == ''):\n",
      "            self.run_on_all_groups = True\n",
      "        else:\n",
      "            self.run_on_all_groups = False        \n",
      "            self.groups = [ g for g in self.parse_cs_ints(self.options.groups) ]\n",
      "            # stderr.write('runnning on groups: ' + str(self.groups))\n",
      "\n",
      "            \n",
      "    \n",
      "    def mapper(self, _, line):\n",
      "        try:\n",
      "            self.increment_counter('MrJob Counters','mapper-all',1)\n",
      "            elements = line.split(',')\n",
      "\n",
      "            # TODO: add support for both TMIN and TMAX\n",
      "            if elements[1] == 'TMAX':\n",
      "                \n",
      "                # station id\n",
      "                st_id = elements[0]\n",
      "\n",
      "                # group_id of the station \n",
      "                group_id = int(self.stations.loc[st_id]['group_id'])\n",
      "                \n",
      "                if self.run_on_all_groups:\n",
      "                    yield (group_id, ','.join(elements[3:]))\n",
      "                else:\n",
      "                    if group_id in self.groups:\n",
      "                        yield (group_id, ','.join(elements[3:]))\n",
      "\n",
      "                    \n",
      "        \n",
      "        except KeyError as e:\n",
      "            stderr.write('Key was not found.\\n')\n",
      "            \n",
      "        except Exception as e:\n",
      "            stderr.write('Error in line:\\n'+line)\n",
      "            #stderr.write(str(e))\n",
      "            self.increment_counter('MrJob Counters','mapper-error',1)\n",
      "            \n",
      "            \n",
      "#     def combiner(self, key, values):\n",
      "#         self.increment_counter('MrJob Counters','combiner',1)                \n",
      "#         yield (key, values)\n",
      "\n",
      "\n",
      "    # PCA on the reducer\n",
      "    \n",
      "    def reducer(self, group_id, values):\n",
      "        self.increment_counter('MrJob Counters','reducer',1)\n",
      "        \n",
      "        def to_int(s):\n",
      "            if s=='': \n",
      "                return np.NaN \n",
      "            else: \n",
      "                return int(s)\n",
      "        \n",
      "        vrows = [ v for v in values ]\n",
      "\n",
      "        \n",
      "        # Dump some output\n",
      "#         stderr.write('### reducer for group: ' + str(group_id) + ' no rows: ' \\\n",
      "#                     + str(len(vrows)) + '\\n')\n",
      "        stderr.write('.')\n",
      "\n",
      "            \n",
      "        # FIXME: \n",
      "        # can we use `np.fromstring('s', dtype=int, sep=',')` here ?\n",
      "        # it seems that the absent value is translated into `0` instead of NaN                \n",
      "        Data=pd.DataFrame([map(to_int, v.split(',')) for v in vrows], columns=range(1,366))\n",
      "        \n",
      "        # Normalize data (from weather notebook)\n",
      "        G=Data.ix[:,1:365]\n",
      "        G[G<-400]=np.NaN\n",
      "        G[G>500]=np.NaN\n",
      "        G=G/10\n",
      "        Data.ix[:,1:365]=G\n",
      "        \n",
      "        from numpy import mean, std\n",
      "        \n",
      "        # Scale by Mean Std\n",
      "        matrix=Data.iloc[:,:]\n",
      "        Dout=Data.loc[:,range(1,366)]\n",
      "        Mean=mean(matrix, axis=1).values\n",
      "        Dout['Mean']=Mean\n",
      "        Std= std(matrix, axis=1).values\n",
      "        Dout['Std']=Std\n",
      "        Dout.loc[:,1:365]=matrix.values\n",
      "        \n",
      "\n",
      "        # Dout = Scale(Data) + 'mean' + 'std'\n",
      "        Dout=Dout[['Mean','Std']+range(1,366)]\n",
      "        \n",
      "        # Calculate covariance\n",
      "        M=Dout.loc[:,1:365].transpose()\n",
      "        M=M.dropna(axis=1)\n",
      "        (columns,rows)=M.shape\n",
      "        Mean=mean(M, axis=1).values\n",
      "        \n",
      "        C=np.zeros([columns,columns])   # Sum\n",
      "        N=np.zeros([columns,columns])   # Counter of non-nan entries\n",
      "        \n",
      "        for i in range(rows):\n",
      "            row = M.iloc[:,i]-Mean;\n",
      "            outer = np.outer(row,row)\n",
      "            valid = np.isnan(outer)==False\n",
      "            C[valid] = C[valid]+outer[valid]  # update C with the valid location in outer\n",
      "            N[valid] = N[valid]+1\n",
      "        cov = np.divide(C,N)\n",
      "        \n",
      " \n",
      "        # SVD\n",
      "        U,D,V=np.linalg.svd(cov)\n",
      "    \n",
      "        \n",
      "        # explained variance\n",
      "        exp_var = np.cumsum(D[:])/np.sum(D)\n",
      "        \n",
      "        \n",
      "#         # Dump the eigenvectors at scratch bucket\n",
      "#         # we transpose U so each row is an eigenvector\n",
      "#         eigpd = pd.DataFrame(U.T)\n",
      "\n",
      "#         import StringIO\n",
      "#         s = StringIO.StringIO()\n",
      "#         eigpd.to_csv(s, index=False)\n",
      "\n",
      "\n",
      "#         from boto.s3.connection import S3Connection\n",
      "#         import boto\n",
      "#         conn = boto.connect_s3(self.options.key, self.options.secret)\n",
      "#         bucket = conn.get_bucket('pvekris.bucket')\n",
      "\n",
      "#         from boto.s3.key import Key\n",
      "#         k = Key(bucket)\n",
      "        \n",
      "#         # group ids are unique so we are not going to overwrite any file\n",
      "#         k.key = 'eigenvectors/eig_' + str(group_id) + '.csv'\n",
      "#         k.set_contents_from_string(s.getvalue())   \n",
      "\n",
      "        \n",
      "        yield (group_id, ','.join([str(len(vrows))] + [str(i) for i in exp_var]))\n",
      "        \n",
      "        # output format\n",
      "        #\n",
      "        # <group_id> \"<no_of_measurement>,<e1>,<e2>,...,<e365>\"\n",
      "        #\n",
      "        # where ei is the explained variance with i eigenvectors\n",
      "        \n",
      "if __name__ == '__main__':\n",
      "    MRPCA.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting pca.py\n"
       ]
      }
     ],
     "prompt_number": 383
    }
   ],
   "metadata": {}
  }
 ]
}