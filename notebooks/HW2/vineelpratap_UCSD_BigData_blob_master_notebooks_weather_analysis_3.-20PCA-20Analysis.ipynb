{
 "metadata": {
  "name": "",
  "signature": "sha256:ea69e8c8b74a3e1e754448985da88256411bd3301bd399ae2259e65d58ef5c63"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font color= 'blue'> Importing modules </font>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os,sys,re,pickle, sklearn, numpy, re, base64, zlib\n",
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 88
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "home_dir='/home/ubuntu/UCSD_BigData'\n",
      "sys.path.append(home_dir+'/utils')\n",
      "\n",
      "from find_waiting_flow import *\n",
      "from AWS_keypair_management import *\n",
      "\n",
      "data_dir=home_dir+'/data/weather'\n",
      "cur_dir =home_dir+'/notebooks/weather_analyis'\n",
      "!ls $data_dir"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ALL.head.csv\t  ghcnd-stations_buffered.txt  SAMPLE_TMAX.csv\r\n",
        "data-source.txt   ghcnd-stations.txt\t       SAMPLE_TMAX.csv.old.gz\r\n",
        "ghcnd-readme.txt  ghcnd-version.txt\r\n"
       ]
      }
     ],
     "prompt_number": 89
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Creds= pickle.load(open('/home/ubuntu/Vault/Creds.pkl','rb'))\n",
      "pair=Creds['mrjob']\n",
      "key_id=pair['key_id']\n",
      "secret_key=pair['secret_key']\n",
      "ID=pair['ID']\n",
      "\n",
      "%cd $home_dir/utils/\n",
      "!python Make.mrjob.conf.py  #If EC2_VAULT is not defined, default location - '/home/ubuntu/Vault'\n",
      "%cd $home_dir/notebooks/weather_analysis"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/ubuntu/UCSD_BigData/utils\n",
        "Created the configuration file: /home/ubuntu/.mrjob.conf\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/ubuntu/UCSD_BigData/notebooks/weather_analysis\n"
       ]
      }
     ],
     "prompt_number": 90
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open('partition_tree.pkl', 'rb') \n",
      "(Data_Stations, valid_years, group_neighbours, group2id) = pickle.load(f) \n",
      "f.close() \n",
      "\n",
      "num_stations = Data_Stations.shape[0]\n",
      "stations = np.array(Data_Stations.index)\n",
      "lons = np.array(Data_Stations.ix[:,'longitude'])\n",
      "lats = np.array(Data_Stations.ix[:,'latitude'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 91
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Collect required data for all the leaves  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile Get_Data.py\n",
      "\n",
      "#!/usr/bin/python\n",
      "\"\"\"\n",
      "collect the data for each group.\n",
      "\"\"\"\n",
      "import re,pickle,base64,zlib\n",
      "from sys import stderr\n",
      "import sys\n",
      "import numpy as np\n",
      "\n",
      "sys.path.append('/usr/lib/python2.6/dist-packages') # a hack because anaconda made mrjob unreachable\n",
      "from mrjob.job import MRJob\n",
      "from mrjob.protocol import *\n",
      "\n",
      "import traceback\n",
      "from functools import wraps\n",
      "from sys import stderr\n",
      "\n",
      "\"\"\"this decorator is intended for decorating a function, not a\n",
      "generator.  Therefore to use it in the context of mrjob, the generator\n",
      "should call a function that handles a single input records, and that\n",
      "function should be decorated.\n",
      "\n",
      "The reason is that if a generator throws an exception it exits and\n",
      "cannot process any more records.\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "import pandas,sys\n",
      "\n",
      "def ECatch(func):\n",
      "    f_name=func.__name__\n",
      "    #@wraps(func) cd\n",
      "    def inner(self,*args,**kwargs):\n",
      "        try:\n",
      "            self.increment_counter(self.__class__.__name__,'total in '+f_name,1)\n",
      "            return func(self,*args,**kwargs)\n",
      "        except Exception as e:\n",
      "            self.increment_counter(self.__class__.__name__,'errors in '+f_name,1)\n",
      "            stderr.write('Error:')\n",
      "            stderr.write(str(e))\n",
      "            traceback.print_exc(file=stderr)\n",
      "            stderr.write('Arguments were %s, %s\\n'%(args,kwargs))\n",
      "            pass\n",
      "    return inner        \n",
      "\n",
      "\"\"\"\n",
      "Functions for encoding and decoding arbitrary object into ascii \n",
      "so that they can be passed through the hadoop streaming interface.\n",
      "\"\"\"\n",
      "def loads(eVal):\n",
      "    \"\"\" Decode a string into a value \"\"\"\n",
      "    return pickle.loads(zlib.decompress(base64.b64decode(eVal)))\n",
      "\n",
      "def dumps(Value):\n",
      "    \"\"\" Encode a value as a string \"\"\"\n",
      "    return base64.b64encode(zlib.compress(pickle.dumps(Value),9))\n",
      "\n",
      "class Get_Data(MRJob):\n",
      "    \n",
      "    #This adds options --stations for the location of where to get the lookup file\n",
      "    def configure_options(self):\n",
      "        super(Get_Data,self).configure_options()\n",
      "        self.add_file_option('--group_data')\n",
      "        \n",
      "    #This loads the lookup file into a field on the object\n",
      "    def load_group_data(self):\n",
      "        f = open(self.options.group_data, \"rb\" )\n",
      "        (self.Data_Stations, self.valid_years, _ , self.group2id) = pickle.load(f) \n",
      "        f.close()\n",
      "               \n",
      "    @ECatch\n",
      "    def map_one(self,line):\n",
      "        return line.split(',')\n",
      "    def mapper_get_station_data(self, _, line):\n",
      "        self.increment_counter('MrJob Counters','mapper',1)\n",
      "        elements=self.map_one(line)\n",
      "        [stn , meas, yr] =  elements[0:3]\n",
      "        if ((stn != 'station') and stn in self.Data_Stations.index and (meas in ['TMIN', 'TMAX']) \\\n",
      "                                                            and (int(yr) in self.valid_years[stn])):\n",
      "            yield(stn, elements[1:])\n",
      "   \n",
      "    \n",
      "    @ECatch\n",
      "    def reducer_get_station_data(self, stn, vector):\n",
      "        D = np.zeros((len(self.valid_years[stn]), 365*2))\n",
      "        for vec in vector:\n",
      "            (meas,yr) = vec[0] , int(vec[1])\n",
      "            \n",
      "            mat = np.zeros(365*2)\n",
      "            mat_id = 365*(meas=='TMIN')\n",
      "            mat[0+mat_id: 365+mat_id] = np.array([(np.nan if j == '' else int(j)) for j in vec[2:]])\n",
      "            \n",
      "            D[self.valid_years[stn].index(yr),:] = D[self.valid_years[stn].index(yr),:] + mat\n",
      "        \n",
      "        grp_id = self.group2id[self.Data_Stations['group_id'][stn]]\n",
      "        yield(grp_id, D.tolist())\n",
      "                         \n",
      "    def reducer_get_group_data(self, grp_id, mat_list):\n",
      "        grp_data = []\n",
      "        for mat in mat_list:\n",
      "            grp_data = grp_data + mat\n",
      "            \n",
      "        yield(grp_id, dumps(np.array(grp_data)))\n",
      "    \n",
      "    def steps(self):\n",
      "        return [\n",
      "            self.mr(mapper_pre_filter='grep \"TMAX\\|TMIN\"',\n",
      "                    mapper_init = self.load_group_data,\n",
      "                    mapper=self.mapper_get_station_data,\n",
      "                    reducer_init = self.load_group_data,\n",
      "                    reducer=self.reducer_get_station_data),\n",
      "            self.mr(reducer=self.reducer_get_group_data)\n",
      "        ]\n",
      "if __name__ == '__main__':\n",
      "    Get_Data.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting Get_Data.py\n"
       ]
      }
     ],
     "prompt_number": 92
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Running on local data\n",
      "!python Get_Data.py -r local --group_data ./partition_tree.pkl  $data_dir/ALL.head.csv  > Data_local"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "using configs in /home/ubuntu/.mrjob.conf\r\n",
        "creating tmp directory /tmp/Get_Data.ubuntu.20140527.044224.187794\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "writing to /tmp/Get_Data.ubuntu.20140527.044224.187794/step-0-mapper_part-00000\r\n",
        "> cat /tmp/Get_Data.ubuntu.20140527.044224.187794/input_part-00000 | grep 'TMAX\\|TMIN' | /home/ubuntu/anaconda/bin/python Get_Data.py --step-num=0 --mapper --group_data partition_tree.pkl > /tmp/Get_Data.ubuntu.20140527.044224.187794/step-0-mapper_part-00000\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "writing to /tmp/Get_Data.ubuntu.20140527.044224.187794/step-0-mapper_part-00001\r\n",
        "> cat /tmp/Get_Data.ubuntu.20140527.044224.187794/input_part-00001 | grep 'TMAX\\|TMIN' | /home/ubuntu/anaconda/bin/python Get_Data.py --step-num=0 --mapper --group_data partition_tree.pkl > /tmp/Get_Data.ubuntu.20140527.044224.187794/step-0-mapper_part-00001\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counters from step 1:\r\n",
        "  Get_Data:\r\n",
        "    total in map_one: 229\r\n",
        "  MrJob Counters:\r\n",
        "    mapper: 229\r\n",
        "writing to /tmp/Get_Data.ubuntu.20140527.044224.187794/step-0-mapper-sorted\r\n",
        "> sort /tmp/Get_Data.ubuntu.20140527.044224.187794/step-0-mapper_part-00000 /tmp/Get_Data.ubuntu.20140527.044224.187794/step-0-mapper_part-00001\r\n",
        "writing to /tmp/Get_Data.ubuntu.20140527.044224.187794/step-0-reducer_part-00000\r\n",
        "> /home/ubuntu/anaconda/bin/python Get_Data.py --step-num=0 --reducer --group_data partition_tree.pkl /tmp/Get_Data.ubuntu.20140527.044224.187794/input_part-00000 > /tmp/Get_Data.ubuntu.20140527.044224.187794/step-0-reducer_part-00000\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "writing to /tmp/Get_Data.ubuntu.20140527.044224.187794/step-0-reducer_part-00001\r\n",
        "> /home/ubuntu/anaconda/bin/python Get_Data.py --step-num=0 --reducer --group_data partition_tree.pkl /tmp/Get_Data.ubuntu.20140527.044224.187794/input_part-00001 > /tmp/Get_Data.ubuntu.20140527.044224.187794/step-0-reducer_part-00001\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counters from step 1:\r\n",
        "  Get_Data:\r\n",
        "    total in map_one: 229\r\n",
        "    total in reducer_get_station_data: 4\r\n",
        "  MrJob Counters:\r\n",
        "    mapper: 229\r\n",
        "writing to /tmp/Get_Data.ubuntu.20140527.044224.187794/step-1-mapper_part-00000\r\n",
        "> cat /tmp/Get_Data.ubuntu.20140527.044224.187794/input_part-00000 | cat > /tmp/Get_Data.ubuntu.20140527.044224.187794/step-1-mapper_part-00000\r\n",
        "writing to /tmp/Get_Data.ubuntu.20140527.044224.187794/step-1-mapper_part-00001\r\n",
        "> cat /tmp/Get_Data.ubuntu.20140527.044224.187794/input_part-00001 | cat > /tmp/Get_Data.ubuntu.20140527.044224.187794/step-1-mapper_part-00001\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counters from step 2:\r\n",
        "  (no counters found)\r\n",
        "writing to /tmp/Get_Data.ubuntu.20140527.044224.187794/step-1-mapper-sorted\r\n",
        "> sort /tmp/Get_Data.ubuntu.20140527.044224.187794/step-1-mapper_part-00000 /tmp/Get_Data.ubuntu.20140527.044224.187794/step-1-mapper_part-00001\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "writing to /tmp/Get_Data.ubuntu.20140527.044224.187794/step-1-reducer_part-00000\r\n",
        "> /home/ubuntu/anaconda/bin/python Get_Data.py --step-num=1 --reducer --group_data partition_tree.pkl /tmp/Get_Data.ubuntu.20140527.044224.187794/input_part-00000 > /tmp/Get_Data.ubuntu.20140527.044224.187794/step-1-reducer_part-00000\r\n",
        "writing to /tmp/Get_Data.ubuntu.20140527.044224.187794/step-1-reducer_part-00001\r\n",
        "> /home/ubuntu/anaconda/bin/python Get_Data.py --step-num=1 --reducer --group_data partition_tree.pkl /tmp/Get_Data.ubuntu.20140527.044224.187794/input_part-00001 > /tmp/Get_Data.ubuntu.20140527.044224.187794/step-1-reducer_part-00001\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counters from step 2:\r\n",
        "  (no counters found)\r\n",
        "Moving /tmp/Get_Data.ubuntu.20140527.044224.187794/step-1-reducer_part-00000 -> /tmp/Get_Data.ubuntu.20140527.044224.187794/output/part-00000\r\n",
        "Moving /tmp/Get_Data.ubuntu.20140527.044224.187794/step-1-reducer_part-00001 -> /tmp/Get_Data.ubuntu.20140527.044224.187794/output/part-00001\r\n",
        "Streaming final output from /tmp/Get_Data.ubuntu.20140527.044224.187794/output\r\n",
        "removing tmp directory /tmp/Get_Data.ubuntu.20140527.044224.187794\r\n"
       ]
      }
     ],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "job_flow_id=find_waiting_flow(key_id,secret_key)\n",
      "!python Get_Data.py -r emr --emr-job-flow-id $job_flow_id --group_data ./partition_tree.pkl  hdfs:/weather/weather.csv > Data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "using configs in /home/ubuntu/.mrjob.conf\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "creating new scratch bucket mrjob-376cc721c54def3b\r\n",
        "using s3://mrjob-376cc721c54def3b/tmp/ as our scratch dir on S3\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "creating tmp directory /tmp/Get_Data.ubuntu.20140527.055550.823419\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "creating S3 bucket 'mrjob-376cc721c54def3b' to use as scratch space\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Copying non-input files into s3://mrjob-376cc721c54def3b/tmp/Get_Data.ubuntu.20140527.055550.823419/files/\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Adding our job to existing job flow NONE\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got retriable error: EmrResponseError: 400 Bad Request\r\n",
        "<ErrorResponse xmlns=\"http://elasticmapreduce.amazonaws.com/doc/2009-03-31\">\r\n",
        "  <Error>\r\n",
        "    <Type>Sender</Type>\r\n",
        "    <Code>Throttling</Code>\r\n",
        "    <Message>Rate exceeded</Message>\r\n",
        "  </Error>\r\n",
        "  <RequestId>802f1992-e563-11e3-80e8-ff6cff06198c</RequestId>\r\n",
        "</ErrorResponse>\r\n",
        "\r\n",
        "Backing off for 20.0 seconds\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got retriable error: EmrResponseError: 400 Bad Request\r\n",
        "<ErrorResponse xmlns=\"http://elasticmapreduce.amazonaws.com/doc/2009-03-31\">\r\n",
        "  <Error>\r\n",
        "    <Type>Sender</Type>\r\n",
        "    <Code>Throttling</Code>\r\n",
        "    <Message>Rate exceeded</Message>\r\n",
        "  </Error>\r\n",
        "  <RequestId>8c1f6f0a-e563-11e3-abd6-21155152508b</RequestId>\r\n",
        "</ErrorResponse>\r\n",
        "\r\n",
        "Backing off for 30.0 seconds\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got retriable error: EmrResponseError: 400 Bad Request\r\n",
        "<ErrorResponse xmlns=\"http://elasticmapreduce.amazonaws.com/doc/2009-03-31\">\r\n",
        "  <Error>\r\n",
        "    <Type>Sender</Type>\r\n",
        "    <Code>Throttling</Code>\r\n",
        "    <Message>Rate exceeded</Message>\r\n",
        "  </Error>\r\n",
        "  <RequestId>9e07a1e5-e563-11e3-9fde-05f53a330f8a</RequestId>\r\n",
        "</ErrorResponse>\r\n",
        "\r\n",
        "Backing off for 45.0 seconds\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got retriable error: EmrResponseError: 400 Bad Request\r\n",
        "<ErrorResponse xmlns=\"http://elasticmapreduce.amazonaws.com/doc/2009-03-31\">\r\n",
        "  <Error>\r\n",
        "    <Type>Sender</Type>\r\n",
        "    <Code>Throttling</Code>\r\n",
        "    <Message>Rate exceeded</Message>\r\n",
        "  </Error>\r\n",
        "  <RequestId>b8e33d9d-e563-11e3-80e8-ff6cff06198c</RequestId>\r\n",
        "</ErrorResponse>\r\n",
        "\r\n",
        "Backing off for 67.5 seconds\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got retriable error: EmrResponseError: 400 Bad Request\r\n",
        "<ErrorResponse xmlns=\"http://elasticmapreduce.amazonaws.com/doc/2009-03-31\">\r\n",
        "  <Error>\r\n",
        "    <Type>Sender</Type>\r\n",
        "    <Code>Throttling</Code>\r\n",
        "    <Message>Rate exceeded</Message>\r\n",
        "  </Error>\r\n",
        "  <RequestId>e1329b98-e563-11e3-b332-21d671b574dc</RequestId>\r\n",
        "</ErrorResponse>\r\n",
        "\r\n",
        "Backing off for 20.0 seconds\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Traceback (most recent call last):\r\n",
        "  File \"Get_Data.py\", line 117, in <module>\r\n",
        "    Get_Data.run()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/job.py\", line 494, in run\r\n",
        "    mr_job.execute()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/job.py\", line 512, in execute\r\n",
        "    super(MRJob, self).execute()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/launch.py\", line 147, in execute\r\n",
        "    self.run_job()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/launch.py\", line 208, in run_job\r\n",
        "    runner.run()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/runner.py\", line 458, in run\r\n",
        "    self._run()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/emr.py\", line 808, in _run\r\n",
        "    self._launch_emr_job()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/emr.py\", line 1454, in _launch_emr_job\r\n",
        "    steps = self._build_steps()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/emr.py\", line 1319, in _build_steps\r\n",
        "    return [self._build_step(n) for n in xrange(self._num_steps())]\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/emr.py\", line 1325, in _build_step\r\n",
        "    return self._build_streaming_step(step_num)\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/emr.py\", line 1341, in _build_streaming_step\r\n",
        "    streaming_step_kwargs.update(self._cache_kwargs())\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/emr.py\", line 1402, in _cache_kwargs\r\n",
        "    version = self.get_hadoop_version()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/emr.py\", line 2419, in get_hadoop_version\r\n",
        "    self._describe_jobflow().hadoopversion)\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/emr.py\", line 2406, in _describe_jobflow\r\n",
        "    return emr_conn.describe_jobflow(self._emr_job_flow_id)\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/retry.py\", line 148, in call_and_maybe_retry\r\n",
        "    return f(*args, **kwargs)\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/retry.py\", line 70, in call_and_maybe_retry\r\n",
        "    result = getattr(alternative, name)(*args, **kwargs)\r\n",
        "  File \"/home/ubuntu/anaconda/lib/python2.7/site-packages/boto/emr/connection.py\", line 103, in describe_jobflow\r\n",
        "    jobflows = self.describe_jobflows(jobflow_ids=[jobflow_id])\r\n",
        "  File \"/home/ubuntu/anaconda/lib/python2.7/site-packages/boto/emr/connection.py\", line 136, in describe_jobflows\r\n",
        "    return self.get_list('DescribeJobFlows', params, [('member', JobFlow)])\r\n",
        "  File \"/home/ubuntu/anaconda/lib/python2.7/site-packages/boto/connection.py\", line 1141, in get_list\r\n",
        "    raise self.ResponseError(response.status, response.reason, body)\r\n",
        "boto.exception.EmrResponseError: EmrResponseError: 400 Bad Request\r\n",
        "<ErrorResponse xmlns=\"http://elasticmapreduce.amazonaws.com/doc/2009-03-31\">\r\n",
        "  <Error>\r\n",
        "    <Type>Sender</Type>\r\n",
        "    <Code>ValidationError</Code>\r\n",
        "    <Message>Specified job flow ID not valid</Message>\r\n",
        "  </Error>\r\n",
        "  <RequestId>e12e07c7-e563-11e3-af71-7d8b78bb4770</RequestId>\r\n",
        "</ErrorResponse>\r\n",
        "\r\n"
       ]
      }
     ],
     "prompt_number": 94
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Computing Covariance for each group"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile Compute_Cov.py\n",
      "\n",
      "#!/usr/bin/python\n",
      "import cPickle as pickle\n",
      "from StringIO import StringIO\n",
      "import numpy as np\n",
      "from mrjob.job import MRJob\n",
      "from mrjob.protocol import PickleValueProtocol\n",
      "\n",
      "class Compute_Cov(MRJob):\n",
      "    INTERNAL_PROTOCOL = PickleValueProtocol\n",
      "    def mapper(self, _, line):\n",
      "        self.increment_counter('MrJob Counters','mapper',1)\n",
      "        xi = np.genfromtxt(StringIO(line),delimiter=\",\")\n",
      "        xi  = np.array(xi, ndmin=2)\n",
      "        cov = np.dot(xi.T, xi) \n",
      "        yield(None,cov)\n",
      "    \n",
      "    def reducer(self, key, cov):\n",
      "        self.increment_counter('MrJob Counters','reducer',1)\n",
      "        s = sum(cov)\n",
      "        print pickle.dumps(s)\n",
      "        yield(None,pickle.dumps(s))\n",
      "    \n",
      "if __name__ == '__main__':\n",
      "    Compute_Cov.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting Compute_Cov.py\n"
       ]
      }
     ],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def loads(eVal):\n",
      "    \"\"\" Decode a string into a value \"\"\"\n",
      "    return pickle.loads(zlib.decompress(base64.b64decode(eVal)))\n",
      "\n",
      "f = open('Data_store')\n",
      "mean_dict = {}\n",
      "mat = []\n",
      "for i in f:\n",
      "    grp , dum = i.split()\n",
      "    print grp\n",
      "    mat =  loads(dum[1:-1])\n",
      "    #print mat\n",
      "    np.savetxt('in_file', mat, delimiter=',')\n",
      "    file_name = 'Data_dir/'+ grp + '_cov.csv'\n",
      "    !python Compute_Cov.py -r local  ./in_file  > out_file\n",
      "    !rm \\in_file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "using configs in /home/ubuntu/.mrjob.conf\r\n",
        "creating tmp directory /tmp/Compute_Cov.ubuntu.20140527.055247.688280\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "writing to /tmp/Compute_Cov.ubuntu.20140527.055247.688280/step-0-mapper_part-00000\r\n",
        "> /home/ubuntu/anaconda/bin/python Compute_Cov.py --step-num=0 --mapper /tmp/Compute_Cov.ubuntu.20140527.055247.688280/input_part-00000 > /tmp/Compute_Cov.ubuntu.20140527.055247.688280/step-0-mapper_part-00000\r\n",
        "writing to /tmp/Compute_Cov.ubuntu.20140527.055247.688280/step-0-mapper_part-00001\r\n",
        "> /home/ubuntu/anaconda/bin/python Compute_Cov.py --step-num=0 --mapper /tmp/Compute_Cov.ubuntu.20140527.055247.688280/input_part-00001 > /tmp/Compute_Cov.ubuntu.20140527.055247.688280/step-0-mapper_part-00001\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "^CTraceback (most recent call last):\r\n",
        "  File \"Compute_Cov.py\", line 25, in <module>\r\n",
        "    Compute_Cov.run()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/job.py\", line 494, in run\r\n",
        "    mr_job.execute()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/job.py\", line 512, in execute\r\n",
        "    super(MRJob, self).execute()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/launch.py\", line 147, in execute\r\n",
        "    self.run_job()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/launch.py\", line 208, in run_job\r\n",
        "    runner.run()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/runner.py\", line 458, in run\r\n",
        "    self._run()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/sim.py\", line 182, in _run\r\n",
        "    self._invoke_step(step_num, 'mapper')\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/sim.py\", line 273, in _invoke_step\r\n",
        "    self.per_step_runner_finish(step_num)\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/local.py\", line 155, in per_step_runner_finish\r\n",
        "    self._wait_for_process(proc_dict, step_num)\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/local.py\", line 261, in _wait_for_process\r\n",
        "    tb_lines = find_python_traceback(stderr_lines)\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/parse.py\", line 185, in find_python_traceback\r\n",
        "    for line in lines:\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/local.py\", line 287, in _process_stderr_from_script\r\n",
        "    parsed = parse_mr_job_stderr(\r\n",
        "KeyboardInterrupt\r\n",
        "\r\n"
       ]
      }
     ],
     "prompt_number": 87
    }
   ],
   "metadata": {}
  }
 ]
}