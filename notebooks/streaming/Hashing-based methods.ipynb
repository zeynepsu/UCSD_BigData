{
 "metadata": {
  "name": "",
  "signature": "sha256:f2c0a9c94ad5b583fe72910ef7cefa0e9a13aa0a5f72863e54501dc8090facfa"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook is based on [chapter 4](http://infolab.stanford.edu/~ullman/mmds/ch4.pdf) of the book [Mining of Massive Datasets](http://infolab.stanford.edu/~ullman/mmds.html#latest) by Anand Rajarman, Jure Leskovec and Jeffrey Ullman."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random as r\n",
      "from Hash import Hash"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Bloom Filters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the Sampling notebook we explored an efficient method for estimating the fraction of elements that appear more than once. Here we discuss a related but harder quesion: how do we estimate the **set** of items that appear more than once. Sampling will not help us here because we will miss all of those items that appeared more than once but were not selected by our Hash function.\n",
      "\n",
      "A classical solution to this are **Bloom Filters** described and analyzed [here](http://seed.ucsd.edu/mediawiki/images/5/52/BloomFilter.pdf).\n",
      "\n",
      "In this section we give an implementation of Bloom filters and show the result of a few experiments (Soon To Come)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random as r\n",
      "s=100000   # number of singles\n",
      "d=1000   # number of doubles\n",
      "\n",
      "ids=permutation(range(s+d))   # the items are random numbers in the range 0 to s+d-1\n",
      "singles=[i for i in ids[:s]]\n",
      "doubles=[[i,i] for i in ids[s:]]\n",
      "doubles=list(concatenate(doubles))\n",
      "L=singles+doubles\n",
      "L=permutation(L)  # Randomly permute the sequence so that sampling can be done by taking the first N elements"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Counting Distinct Items"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random as r\n",
      "num_id=800 # the number of distinct elements\n",
      "ids=[int(r*1e10) for r in random.rand(num_id)]\n",
      "print 'length of sequence',len(ids),'number of different ids',len(unique(ids))\n",
      "L=[]\n",
      "for id in ids:\n",
      "    copies = int(rand()*10)\n",
      "    L = L+[id]*copies\n",
      "L=permutation(L)  # Randomly permute the sequence to \"hide\" the doubles"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "length of sequence 800 number of different ids 800\n"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def count_zeros(n):\n",
      "    bits=int(ceil(log(n)/log(2)))+1\n",
      "    #print n,bits\n",
      "    for i in range(1,bits+1):\n",
      "        if n % (2**i) != 0:\n",
      "            return i-1\n",
      "count_zeros(12)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 63,
       "text": [
        "2"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Flajolet_martin(sequence):\n",
      "    H=Hash(range=1e12)\n",
      "    max_zeros=0\n",
      "    for x in L:\n",
      "        zeros=count_zeros(H.map(x))\n",
      "        max_zeros=max(max_zeros,zeros)\n",
      "    return 2**max_zeros"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "M=30    # Number of groups (mean over groups) \n",
      "N=10       # Size of each group (median inside group)\n",
      "Estimates=np.zeros(N)\n",
      "Medians=np.zeros(M)\n",
      "for i in range(M):\n",
      "    for j in range(N):\n",
      "        Estimates[j]=Flajolet_martin(L)\n",
      "    print '%4d:'%i,','.join(['%5d'%e for e in Estimates]),\n",
      "    Medians[i]=median(Estimates)\n",
      "    print '\\t',Medians[i]\n",
      "Final=mean(Medians)\n",
      "print Final"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   0:  8192,16384, 8192,  512,16384,  512, 2048,  128, 8192, 1024 \t5120.0\n",
        "   1:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  4096,  128,  512,  256,  256, 1024,  256,  128,  256, 8192 \t256.0\n",
        "   2:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   512, 2048,  512, 1024, 4096,  512,  512, 1024,  256,  512 \t512.0\n",
        "   3:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   512,  512,  256, 1024,  512,  512,  512,  256,  512, 4096 \t512.0\n",
        "   4:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  4096, 1024, 8192, 2048, 1024, 1024, 1024,  512, 1024,16384 \t1024.0\n",
        "   5:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  2048,  256,  256, 1024, 2048,  256,131072,  512,  512,  512 \t512.0\n",
        "   6:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   512,  512,  512,  128,  256,16384, 4096,  256, 2048, 8192 \t512.0\n",
        "   7:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  8192,  256, 4096, 8192, 8192, 4096,  256,  512, 2048,  512 \t3072.0\n",
        "   8:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   256,  256, 1024, 1024,  256, 1024,  512, 2048,  512,  512 \t512.0\n",
        "   9:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  2048, 2048,  512, 2048, 1024, 1024, 1024, 1024,  512, 2048 \t1024.0\n",
        "  10:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  1024, 1024,  256, 1024, 1024, 8192,  512,16384,32768, 2048 \t1024.0\n",
        "  11:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   512,  256,  128,32768,  256,  512,  256, 4096, 1024, 4096 \t512.0\n",
        "  12:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  1024, 1024,  256,  256,  512, 2048, 1024,  512, 1024, 4096 \t1024.0\n",
        "  13:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   512,  128,  256,  512, 1024, 2048,  256,  256, 8192,  512 \t512.0\n",
        "  14:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  1024, 1024, 4096,  128,  512, 4096,  512, 2048, 1024,  512 \t1024.0\n",
        "  15:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   512,  256,  512, 4096,  256,16384, 2048, 1024,  256,  256 \t512.0\n",
        "  16:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   512,16384,   64,  128, 2048,65536, 4096, 2048, 1024, 2048 \t2048.0\n",
        "  17:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   512,16384, 4096,  128,131072,  512, 2048, 4096,  512,  512 \t1280.0\n",
        "  18:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   256, 4096,  512, 1024, 4096, 1024,  128,  512, 2048,  512 \t768.0\n",
        "  19:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  2048,  256,  512,  512,  512,  512,  256, 1024, 8192, 2048 \t512.0\n",
        "  20:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  4096,  128,  256, 2048, 2048, 4096, 4096, 2048,  256,  128 \t2048.0\n",
        "  21:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  1024, 1024, 1024, 1024,  512, 1024,  512,  512, 1024, 2048 \t1024.0\n",
        "  22:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   512, 8192,  512, 1024,16384,  512,  128,  128,16384,  128 \t512.0\n",
        "  23:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   128, 1024, 8192,  512, 1024,  512, 2048, 1024,  512,  512 \t768.0\n",
        "  24:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   256,  512, 1024,  512, 4096,  256, 2048,  256,  512, 2048 \t512.0\n",
        "  25:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   256,  512,  512, 1024,  256,   64,  128, 2048, 2048,  512 \t512.0\n",
        "  26:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   256,  128, 1024, 1024, 1024, 2048,16384,  512,  512,  256 \t768.0\n",
        "  27:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   256,  512, 2048,  128,  512,32768, 1024, 2048,  256, 2048 \t768.0\n",
        "  28:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   512,  512, 2048,  512, 4096,  512,  512,  256,  256, 1024 \t512.0\n",
        "  29:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  4096,  512,  256, 2048, 2048,  512,  256, 1024,  512,  256 \t512.0\n",
        "1006.93333333\n"
       ]
      }
     ],
     "prompt_number": 78
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Comparing a large number of of documents"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Suppose we have found a paragraph that we suspect has been copied from Moby-Dick. How can we quickly determine whether or not the paragraph is in the book? Can we do it even if our paragraph has omissions, additions and other mistakes relative to the paragraph in the book?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dir='../../data/text'\n",
      "!ls $dir"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CorruptedParagraph.txt  CorruptedParagraph.txt~ Moby-Dick.txt           OneParagraph.txt\r\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "class MinHash:\n",
      "    Large=10**10\n",
      "    def __init__(self,n=10):\n",
      "        self.n=n\n",
      "        self.H=[]\n",
      "        for i in range(n):\n",
      "            self.H.append(Hash(range=self.Large))\n",
      "    def Compute_sketch(self,list):\n",
      "        sketch=[self.Large]*self.n\n",
      "        for e in list:\n",
      "            for i in range(self.n):\n",
      "                hval=self.H[i].map(e)\n",
      "                #print i,sketch[i],hval\n",
      "                sketch[i] = min(sketch[i],hval)\n",
      "        return sketch\n",
      "def Compare_sketches(sketch1,sketch2):\n",
      "    if len(sketch1) != len(sketch2):\n",
      "        sys.exit('Error, sketches are of different lengths',len(sketch1),len(sketch2))\n",
      "    overlap=0\n",
      "    for i in range(len(sketch1)):\n",
      "        if sketch1[i] == sketch2[i]:\n",
      "            overlap+=1\n",
      "    return overlap"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "file=open(dir+'/Moby-Dick.txt','r')\n",
      "count=0\n",
      "paragraph=''\n",
      "minhash=MinHash()\n",
      "for line in file.readlines():\n",
      "    #print len(line)\n",
      "    paragraph += line\n",
      "    if len(line)<3:\n",
      "        if count >37:\n",
      "            print '-------------------------------- count=',count\n",
      "            print paragraph\n",
      "            words=re.split('\\W+',paragraph)\n",
      "            words=[word.lower() for word in words]\n",
      "            print '====== Split Into Words'\n",
      "            print ','.join(words)\n",
      "            print 'minhash=',',',minhash.Compute_sketch(words)\n",
      "        paragraph=''\n",
      "        count +=1\n",
      "        if count>60:\n",
      "            break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-------------------------------- count= 38\n",
        "EXTRACTS (Supplied by a Sub-Sub-Librarian).\r\n",
        "\r\n",
        "\n",
        "====== Split Into Words\n",
        "extracts,supplied,by,a,sub,sub,librarian,\n",
        "minhash= , [2836029048, 728367302, 336549043, 2068367893, 1990038657, 724839339, 579106806, 305077781, 350274783, 617069139]\n",
        "-------------------------------- count= 39\n",
        "It will be seen that this mere painstaking burrower and grub-worm of a\r\n",
        "poor devil of a Sub-Sub appears to have gone through the long Vaticans\r\n",
        "and street-stalls of the earth, picking up whatever random allusions to\r\n",
        "whales he could anyways find in any book whatsoever, sacred or\r\n",
        "profane. Therefore you must not, in every case at least, take the\r\n",
        "higgledy-piggledy whale statements, however authentic, in these\r\n",
        "extracts, for veritable gospel cetology. Far from it. As touching the\r\n",
        "ancient authors generally, as well as the poets here appearing, these\r\n",
        "extracts are solely valuable or entertaining, as affording a glancing\r\n",
        "bird's eye view of what has been promiscuously said, thought, fancied,\r\n",
        "and sung of Leviathan, by many nations and generations, including our\r\n",
        "own.\r\n",
        "\r\n",
        "\n",
        "====== Split Into Words\n",
        "it,will,be,seen,that,this,mere,painstaking,burrower,and,grub,worm,of,a,poor,devil,of,a,sub,sub,appears,to,have,gone,through,the,long,vaticans,and,street,stalls,of,the,earth,picking,up,whatever,random,allusions,to,whales,he,could,anyways,find,in,any,book,whatsoever,sacred,or,profane,therefore,you,must,not,in,every,case,at,least,take,the,higgledy,piggledy,whale,statements,however,authentic,in,these,extracts,for,veritable,gospel,cetology,far,from,it,as,touching,the,ancient,authors,generally,as,well,as,the,poets,here,appearing,these,extracts,are,solely,valuable,or,entertaining,as,affording,a,glancing,bird,s,eye,view,of,what,has,been,promiscuously,said,thought,fancied,and,sung,of,leviathan,by,many,nations,and,generations,including,our,own,\n",
        "minhash= , [176873608, 3155948, 150557075, 77930638, 97261903, 81101304, 22395174, 129441679, 299157399, 15044487]\n",
        "-------------------------------- count= 40\n",
        "So fare thee well, poor devil of a Sub-Sub, whose commentator I am. Thou\r\n",
        "belongest to that hopeless, sallow tribe which no wine of this world\r\n",
        "will ever warm; and for whom even Pale Sherry would be too rosy-strong;\r\n",
        "but with whom one sometimes loves to sit, and feel poor-devilish, too;\r\n",
        "and grow convivial upon tears; and say to them bluntly, with full eyes\r\n",
        "and empty glasses, and in not altogether unpleasant sadness--Give it up,\r\n",
        "Sub-Subs! For by how much the more pains ye take to please the world,\r\n",
        "by so much the more shall ye for ever go thankless! Would that I could\r\n",
        "clear out Hampton Court and the Tuileries for ye! But gulp down your\r\n",
        "tears and hie aloft to the royal-mast with your hearts; for your friends\r\n",
        "who have gone before are clearing out the seven-storied heavens, and\r\n",
        "making refugees of long-pampered Gabriel, Michael, and Raphael, against\r\n",
        "your coming. Here ye strike but splintered hearts together--there, ye\r\n",
        "shall strike unsplinterable glasses!\r\n",
        "\r\n",
        "\n",
        "====== Split Into Words\n",
        "so,fare,thee,well,poor,devil,of,a,sub,sub,whose,commentator,i,am,thou,belongest,to,that,hopeless,sallow,tribe,which,no,wine,of,this,world,will,ever,warm,and,for,whom,even,pale,sherry,would,be,too,rosy,strong,but,with,whom,one,sometimes,loves,to,sit,and,feel,poor,devilish,too,and,grow,convivial,upon,tears,and,say,to,them,bluntly,with,full,eyes,and,empty,glasses,and,in,not,altogether,unpleasant,sadness,give,it,up,sub,subs,for,by,how,much,the,more,pains,ye,take,to,please,the,world,by,so,much,the,more,shall,ye,for,ever,go,thankless,would,that,i,could,clear,out,hampton,court,and,the,tuileries,for,ye,but,gulp,down,your,tears,and,hie,aloft,to,the,royal,mast,with,your,hearts,for,your,friends,who,have,gone,before,are,clearing,out,the,seven,storied,heavens,and,making,refugees,of,long,pampered,gabriel,michael,and,raphael,against,your,coming,here,ye,strike,but,splintered,hearts,together,there,ye,shall,strike,unsplinterable,glasses,\n",
        "minhash= , [12920855, 21663644, 34191222, 119907194, 21936614, 29579478, 579106806, 305077781, 24164001, 15044487]\n",
        "-------------------------------- count= 41\n",
        "\r\n",
        "\n",
        "====== Split Into Words\n",
        ",\n",
        "minhash= , [3539419169, 5665908446, 6684373620, 4456897485, 4911179988, 724839339, 579106806, 479441141, 6575382118, 617069139]\n",
        "-------------------------------- count= 42\n",
        "EXTRACTS.\r\n",
        "\r\n",
        "\n",
        "====== Split Into Words\n",
        "extracts,\n",
        "minhash= , [3539419169, 728367302, 2236927153, 4456897485, 1990038657, 724839339, 579106806, 479441141, 6575382118, 617069139]\n",
        "-------------------------------- count= 43\n",
        "\"And God created great whales.\" --GENESIS.\r\n",
        "\r\n",
        "\n",
        "====== Split Into Words\n",
        ",and,god,created,great,whales,genesis,\n",
        "minhash= , [3539419169, 9867321, 3041343027, 1518074425, 784183672, 724839339, 579106806, 218448162, 1498127043, 617069139]\n",
        "-------------------------------- count= 44\n",
        "\"Leviathan maketh a path to shine after him; One would think the deep to\r\n",
        "be hoary.\" --JOB.\r\n",
        "\r\n",
        "\n",
        "====== Split Into Words\n",
        ",leviathan,maketh,a,path,to,shine,after,him,one,would,think,the,deep,to,be,hoary,job,\n",
        "minhash= , [1280583318, 582074348, 34191222, 1354310991, 851592252, 724839339, 264594080, 83185828, 24164001, 15044487]\n",
        "-------------------------------- count= 45\n",
        "\"Now the Lord had prepared a great fish to swallow up Jonah.\" --JONAH.\r\n",
        "\r\n",
        "\n",
        "====== Split Into Words\n",
        ",now,the,lord,had,prepared,a,great,fish,to,swallow,up,jonah,jonah,\n",
        "minhash= , [647202164, 582074348, 2653124606, 1749914301, 684000443, 79324470, 579106806, 353043887, 361494971, 15044487]\n",
        "-------------------------------- count= 46\n",
        "\"There go the ships; there is that Leviathan whom thou hast made to play\r\n",
        "therein.\" --PSALMS.\r\n",
        "\r\n",
        "\n",
        "====== Split Into Words\n",
        ",there,go,the,ships,there,is,that,leviathan,whom,thou,hast,made,to,play,therein,psalms,\n",
        "minhash= , "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[86006901, 530881990, 444490507, 72706885, 2289907047, 724839339, 563021428, 353043887, 982855689, 15044487]\n",
        "-------------------------------- count= 47\n",
        "\"In that day, the Lord with his sore, and great, and strong sword,\r\n",
        "shall punish Leviathan the piercing serpent, even Leviathan that crooked\r\n",
        "serpent; and he shall slay the dragon that is in the sea.\" --ISAIAH\r\n",
        "\r\n",
        "\n",
        "====== Split Into Words\n",
        ",in,that,day,the,lord,with,his,sore,and,great,and,strong,sword,shall,punish,leviathan,the,piercing,serpent,even,leviathan,that,crooked,serpent,and,he,shall,slay,the,dragon,that,is,in,the,sea,isaiah,\n",
        "minhash= , [952870965, 25086259, 404775128, 147294198, 213361019, 724839339, 579106806, 353043887, 22190144, 617069139]\n",
        "-------------------------------- count= 48\n",
        "\"And what thing soever besides cometh within the chaos of this monster's\r\n",
        "mouth, be it beast, boat, or stone, down it goes all incontinently that\r\n",
        "foul great swallow of his, and perisheth in the bottomless gulf of his\r\n",
        "paunch.\" --HOLLAND'S PLUTARCH'S MORALS.\r\n",
        "\r\n",
        "\n",
        "====== Split Into Words\n",
        ",and,what,thing,soever,besides,cometh,within,the,chaos,of,this,monster,s,mouth,be,it,beast,boat,or,stone,down,it,goes,all,incontinently,that,foul,great,swallow,of,his,and,perisheth,in,the,bottomless,gulf,of,his,paunch,holland,s,plutarch,s,morals,\n",
        "minhash= , [201709667, 617917105, 95525676, 79450926, 291416338, 29579478, 22395174, 34619774, 361494971, 167333414]\n",
        "-------------------------------- count= 49\n",
        "\"The Indian Sea breedeth the most and the biggest fishes that are: among\r\n",
        "which the Whales and Whirlpooles called Balaene, take up as much in\r\n",
        "length as four acres or arpens of land.\" --HOLLAND'S PLINY.\r\n",
        "\r\n",
        "\n",
        "====== Split Into Words\n",
        ",the,indian,sea,breedeth,the,most,and,the,biggest,fishes,that,are,among,which,the,whales,and,whirlpooles,called,balaene,take,up,as,much,in,length,as,four,acres,or,arpens,of,land,holland,s,pliny,\n",
        "minhash= , [113421364, 9867321, 264415143, 81051205, 741708067, 108897420, 22395174, 335069546, 373684461, 61862572]\n",
        "-------------------------------- count= 50\n",
        "\"Scarcely had we proceeded two days on the sea, when about sunrise a\r\n",
        "great many Whales and other monsters of the sea, appeared. Among the\r\n",
        "former, one was of a most monstrous size.... This came towards us,\r\n",
        "open-mouthed, raising the waves on all sides, and beating the sea before\r\n",
        "him into a foam.\" --TOOKE'S LUCIAN. \"THE TRUE HISTORY.\"\r\n",
        "\r\n",
        "\n",
        "====== Split Into Words\n",
        ",scarcely,had,we,proceeded,two,days,on,the,sea,when,about,sunrise,a,great,many,whales,and,other,monsters,of,the,sea,appeared,among,the,former,one,was,of,a,most,monstrous,size,this,came,towards,us,open,mouthed,raising,the,waves,on,all,sides,and,beating,the,sea,before,him,into,a,foam,tooke,s,lucian,the,true,history,\n",
        "minhash= , [194688259, 9867321, 2332869, 152807828, 119707932, 31695685, 114943413, 244999932, 6600769, 118407782]\n",
        "-------------------------------- count= 51\n",
        "\"He visited this country also with a view of catching horse-whales,\r\n",
        "which had bones of very great value for their teeth, of which he brought\r\n",
        "some to the king.... The best whales were catched in his own country, of\r\n",
        "which some were forty-eight, some fifty yards long. He said that he was\r\n",
        "one of six who had killed sixty in two days.\" --OTHER OR OTHER'S VERBAL\r\n",
        "NARRATIVE TAKEN DOWN FROM HIS MOUTH BY KING ALFRED, A.D. 890.\r\n",
        "\r\n",
        "\n",
        "====== Split Into Words\n",
        ",he,visited,this,country,also,with,a,view,of,catching,horse,whales,which,had,bones,of,very,great,value,for,their,teeth,of,which,he,brought,some,to,the,king,the,best,whales,were,catched,in,his,own,country,of,which,some,were,forty,eight,some,fifty,yards,long,he,said,that,he,was,one,of,six,who,had,killed,sixty,in,two,days,other,or,other,s,verbal,narrative,taken,down,from,his,mouth,by,king,alfred,a,d,890,\n",
        "minhash= , [67102929, 9867321, 34191222, 143031172, 54427284, 29579478, 22395174, 278937917, 373684461, 15044487]\n",
        "-------------------------------- count= 52\n",
        "\"And whereas all the other things, whether beast or vessel, that\r\n",
        "enter into the dreadful gulf of this monster's (whale's) mouth, are\r\n",
        "immediately lost and swallowed up, the sea-gudgeon retires into it in\r\n",
        "great security, and there sleeps.\" --MONTAIGNE. --APOLOGY FOR RAIMOND\r\n",
        "SEBOND.\r\n",
        "\r\n",
        "\n",
        "====== Split Into Words\n",
        ",and,whereas,all,the,other,things,whether,beast,or,vessel,that,enter,into,the,dreadful,gulf,of,this,monster,s,whale,s,mouth,are,immediately,lost,and,swallowed,up,the,sea,gudgeon,retires,into,it,in,great,security,and,there,sleeps,montaigne,apology,for,raimond,sebond,\n",
        "minhash= , [234854692, 153142282, 132096817, 59997487, 176277104, 209110847, 22395174, 171330505, 771452550, 19172019]\n",
        "-------------------------------- count= 53\n",
        "\"Let us fly, let us fly! Old Nick take me if is not Leviathan described\r\n",
        "by the noble prophet Moses in the life of patient Job.\" --RABELAIS.\r\n",
        "\r\n",
        "\n",
        "====== Split Into Words\n",
        ",let,us,fly,let,us,fly,old,nick,take,me,if,is,not,leviathan,described,by,the,noble,prophet,moses,in,the,life,of,patient,job,rabelais,\n",
        "minhash= , [909158656, 946702237, 336549043, 1029785930, 67594314, 254523142, 579106806, 305077781, 701485816, 24220451]\n",
        "-------------------------------- count= 54\n",
        "\"This whale's liver was two cartloads.\" --STOWE'S ANNALS.\r\n",
        "\r\n",
        "\n",
        "====== Split Into Words\n",
        ",this,whale,s,liver,was,two,cartloads,stowe,s,annals,\n",
        "minhash= , [194688259, 617917105, 650137787, 201716854, 1912919297, 380943242, 579106806, 479441141, 2122013004, 617069139]\n",
        "-------------------------------- count= 55\n",
        "\"The great Leviathan that maketh the seas to seethe like boiling pan.\"\r\n",
        "--LORD BACON'S VERSION OF THE PSALMS.\r\n",
        "\r\n",
        "\n",
        "====== Split Into Words\n",
        ",the,great,leviathan,that,maketh,the,seas,to,seethe,like,boiling,pan,lord,bacon,s,version,of,the,psalms,\n",
        "minhash= , [110386815, 582074348, 650137787, 201716854, 1515528167, 347413440, 488109425, 353043887, 923085058, 15044487]\n",
        "-------------------------------- count= 56\n",
        "\"Touching that monstrous bulk of the whale or ork we have received\r\n",
        "nothing certain. They grow exceeding fat, insomuch that an incredible\r\n",
        "quantity of oil will be extracted out of one whale.\" --IBID. \"HISTORY OF\r\n",
        "LIFE AND DEATH.\"\r\n",
        "\r\n",
        "\n",
        "====== Split Into Words\n",
        ",touching,that,monstrous,bulk,of,the,whale,or,ork,we,have,received,nothing,certain,they,grow,exceeding,fat,insomuch,that,an,incredible,quantity,of,oil,will,be,extracted,out,of,one,whale,ibid,history,of,life,and,death,\n",
        "minhash= , "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[66505407, 234970679, 34191222, 497506253, 37015964, 92000598, 22395174, 353043887, 433112444, 321047315]\n",
        "-------------------------------- count= 57\n",
        "\"The sovereignest thing on earth is parmacetti for an inward bruise.\"\r\n",
        "--KING HENRY.\r\n",
        "\r\n",
        "\n",
        "====== Split Into Words\n",
        ",the,sovereignest,thing,on,earth,is,parmacetti,for,an,inward,bruise,king,henry,\n",
        "minhash= , [2185619909, 1100625702, 821320025, 1147388104, 192031641, 724839339, 524058024, 353043887, 381174071, 617069139]\n",
        "-------------------------------- count= 58\n",
        "\"Very like a whale.\" --HAMLET.\r\n",
        "\r\n",
        "\n",
        "====== Split Into Words\n",
        ",very,like,a,whale,hamlet,\n",
        "minhash= , [560094178, 1450451614, 1312871431, 4456897485, 2052036447, 724839339, 579106806, 479441141, 1871648677, 617069139]\n",
        "-------------------------------- count= 59\n",
        "     \"Which to secure, no skill of leach's art\r\n",
        "     Mote him availle, but to returne againe\r\n",
        "     To his wound's worker, that with lowly dart,\r\n",
        "     Dinting his breast, had bred his restless paine,\r\n",
        "     Like as the wounded whale to shore flies thro' the maine.\"\r\n",
        "     --THE FAERIE QUEEN.\r\n",
        "\r\n",
        "\n",
        "====== Split Into Words\n",
        ",which,to,secure,no,skill,of,leach,s,art,mote,him,availle,but,to,returne,againe,to,his,wound,s,worker,that,with,lowly,dart,dinting,his,breast,had,bred,his,restless,paine,like,as,the,wounded,whale,to,shore,flies,thro,the,maine,the,faerie,queen,\n",
        "minhash= , [24105627, 243967160, 263625191, 201716854, 111972168, 66452644, 579106806, 121272800, 236276404, 15044487]\n",
        "-------------------------------- count= 60\n",
        "\"Immense as whales, the motion of whose vast bodies can in a peaceful\r\n",
        "calm trouble the ocean til it boil.\" --SIR WILLIAM DAVENANT. PREFACE TO\r\n",
        "GONDIBERT.\r\n",
        "\r\n",
        "\n",
        "====== Split Into Words\n",
        ",immense,as,whales,the,motion,of,whose,vast,bodies,can,in,a,peaceful,calm,trouble,the,ocean,til,it,boil,sir,william,davenant,preface,to,gondibert,\n",
        "minhash= , [141553106, 9867321, 434186928, 154727898, 42414063, 233134787, 433725618, 209259246, 536384228, 15044487]\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re,pickle\n",
      "file=open(dir+'/Moby-Dick.txt','r')\n",
      "hash_list=[] # a table that holds the min-hash sketch for each paragraph\n",
      "offset_list=[] # a table that holds the offset to the start of each paragraph.\n",
      "\n",
      "count=0\n",
      "paragraph=''\n",
      "minhash=MinHash()\n",
      "line_offset=0\n",
      "paragraph_start_offset=0\n",
      "for line in file.readlines():\n",
      "    if len(line)<3:   # found end of paragraph\n",
      "        if len(paragraph)>50:  # ignore paragraphs shorter than 50 chars\n",
      "            offset_list.append(paragraph_start_offset)\n",
      "            words=re.split('\\W+',paragraph)\n",
      "            words=[word.lower() for word in words]\n",
      "            hash_list.append(minhash.Compute_sketch(words))\n",
      "        paragraph=''\n",
      "        paragraph_start_offset=line_offset+len(line)\n",
      "    else:\n",
      "        paragraph += line\n",
      "    if count % 1000==0:\n",
      "        print count,line_offset,len(offset_list),len(hash_list)\n",
      "    #if count >10000:\n",
      "    #    break\n",
      "    count += 1\n",
      "    line_offset += len(line)\n",
      "file.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 0 0 0\n",
        "1000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 51479 134 134\n",
        "2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 109120 241 241\n",
        "3000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 170134 318 318\n",
        "4000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 223265 465 465\n",
        "5000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 279378 569 569\n",
        "6000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 339751 652 652\n",
        "7000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 394010 770 770\n",
        "8000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 454827 849 849\n",
        "9000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 513020 947 947\n",
        "10000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 572511 1040 1040\n",
        "11000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 629854 1150 1150\n",
        "12000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 685926 1260 1260\n",
        "13000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 741837 1380 1380\n",
        "14000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 801258 1473 1473\n",
        "15000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 862702 1549 1549\n",
        "16000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 918280 1667 1667\n",
        "17000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 976933 1763 1763\n",
        "18000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1034549 1861 1861\n",
        "19000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1088917 1990 1990\n",
        "20000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1140214 2135 2135\n",
        "21000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1196310 2251 2251\n",
        "22000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1252126 2378 2378\n"
       ]
      }
     ],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump({'hash_list':hash_list,'offset_list':offset_list},open(dir+'/Moby-Dick.pkl','wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "file=open(dir+'/Moby-Dick.txt','r')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 84
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "item=160\n",
      "file.seek(offset_list[item])\n",
      "for line in file.readlines():\n",
      "    print line,\n",
      "    if len(line)<3: break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\"Landlord,\" said I, going up to him as cool as Mt. Hecla in a\r\n",
        "snow-storm--\"landlord, stop whittling. You and I must understand one\r\n",
        "another, and that too without delay. I come to your house and want a\r\n",
        "bed; you tell me you can only give me half a one; that the other half\r\n",
        "belongs to a certain harpooneer. And about this harpooneer, whom I\r\n",
        "have not yet seen, you persist in telling me the most mystifying and\r\n",
        "exasperating stories tending to beget in me an uncomfortable feeling\r\n",
        "towards the man whom you design for my bedfellow--a sort of connexion,\r\n",
        "landlord, which is an intimate and confidential one in the highest\r\n",
        "degree. I now demand of you to speak out and tell me who and what this\r\n",
        "harpooneer is, and whether I shall be in all respects safe to spend the\r\n",
        "night with him. And in the first place, you will be so good as to unsay\r\n",
        "that story about selling his head, which if true I take to be good\r\n",
        "evidence that this harpooneer is stark mad, and I've no idea of sleeping\r\n",
        "with a madman; and you, sir, YOU I mean, landlord, YOU, sir, by trying\r\n",
        "to induce me to do so knowingly, would thereby render yourself liable to\r\n",
        "a criminal prosecution.\"\r\n",
        "\r\n"
       ]
      }
     ],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print hash_list[item]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[4021482, 54507519, 35242999, 155615148, 11104477, 19090601, 11818312, 11031537, 167233021, 65189349]\n"
       ]
      }
     ],
     "prompt_number": 86
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_minhash(paragraph,hash1):\n",
      "    words=re.split('\\W+',paragraph)\n",
      "    words=[word.lower() for word in words]\n",
      "    my_hash=minhash.Compute_sketch(words)\n",
      "    #print my_hash\n",
      "    #print hash1\n",
      "    return Compare_sketches(my_hash,hash1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 90
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "paragraph=\"\"\"\n",
      "\"Landlord,\" said I, going up to him as cool as Mt. Hecla in a\n",
      "snow-storm--\"landlord, stop whittling. You and I must understand one\n",
      "another, and that too without delay. I come to your house and want a\n",
      "bed; you tell me you can only give me half a one; that the other half\n",
      "belongs to a certain harpooneer. And about this harpooneer, whom I\n",
      "have not yet seen, you persist in telling me the most mystifying and\n",
      "exasperating stories tending to beget in me an uncomfortable feeling\n",
      "towards the man whom you design for my bedfellow--a sort of connexion,\n",
      "landlord, which is an intimate and confidential one in the highest\n",
      "degree. I now demand of you to speak out and tell me who and what this\n",
      "harpooneer is, and whether I shall be in all respects safe to spend the\n",
      "night with him. And in the first place, you will be so good as to unsay\n",
      "that story about selling his head, which if true I take to be good\n",
      "evidence that this harpooneer is stark mad, and I've no idea of sleeping\n",
      "with a madman; and you, sir, YOU I mean, landlord, YOU, sir, by trying\n",
      "to induce me to do so knowingly, would thereby render yourself liable to\n",
      "a criminal prosecution.\"\n",
      "\"\"\"\n",
      "compute_minhash(paragraph,hash_list[160])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[4021482, 54507519, 35242999, 155615148, 11104477, 19090601, 11818312, 11031537, 167233021, 65189349]\n",
        "[4021482, 54507519, 35242999, 155615148, 11104477, 19090601, 11818312, 11031537, 167233021, 65189349]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 89,
       "text": [
        "10"
       ]
      }
     ],
     "prompt_number": 89
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "paragraph=\"\"\"\n",
      "\"Landlord,\" said I, going up to him as cool as Mt. Hecla in a\n",
      "snow-storm--\"landlord, stop whittling. You and I must understand one\n",
      "another, and that  in telling me the most mystifying and\n",
      "exasperating stories tending to beget in me an uncomfortable feeling\n",
      "towards the man whom you design for my bedfellow--a sort of connexion,\n",
      "landlord, which is an intimate and confidential one in the highest\n",
      "degree. I now demand of you to speak badgo kurliano masherabenu \n",
      "out and tell me who and what this\n",
      "\"\"\"\n",
      "compute_minhash(paragraph,hash_list[160])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 93,
       "text": [
        "5"
       ]
      }
     ],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 92
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}